{
	"name": "Procesar Data Cruda PDF - Zona Data - Fondo de Inversion",
	"properties": {
		"folder": {
			"name": "LecturaScripts"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "spproymaestria",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "e45c91fc-1e8c-4936-b1e2-1270f8b4cb1e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
				"name": "spproymaestria",
				"type": "Spark",
				"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"#Importar Librerias Necesarias\r\n",
					"\r\n",
					"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\r\n",
					"from pyspark.sql.window import Window\r\n",
					"from pyspark.sql.functions import *"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"#Configurar parametros aqui\r\n",
					"\r\n",
					"#user = \"bsmoralesg@outlook.com\"\r\n",
					"#type = \"Fondos de Inversion\"\r\n",
					"#fecha = \"10012023\"\r\n",
					"\r\n",
					"#name = \"Extracto_8791222_202108_FIDUCUENTA_3940\"\r\n",
					"#name = \"Extracto_36546412_202111_FIDUCUENTA_3940\"\r\n",
					"#name = \"Extracto_212010590_202208_FIDUCUENTA_3940\"\r\n",
					"#name = \"Extracto_254154275_202210_FIDUCUENTA_3940\"\r\n",
					"#name = \"Extracto_333733883_202301_FIDUCUENTA_3940\"\r\n",
					"#name = \"Extracto_395097292_202304_FIDUCUENTA_3940\"\r\n",
					"\r\n",
					"user = \"bsmoralesg@outlook.com\"\r\n",
					"type = \"Fondos de Inversion\"\r\n",
					"fecha = \"2023-10-18\"\r\n",
					"name = \"Extracto_461173115_202309_FIDUCUENTA_3940\"\r\n",
					"\r\n",
					""
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Configurar rutas de origen de archivos\r\n",
					"\r\n",
					"pathsource = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net/\"+type+\"/Raw/\"+fecha+\"/\"+user+\"/\"+name"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Obtener Datos para Fondo de inversion\r\n",
					"\r\n",
					"DataCruda = spark.read.load(pathsource+\"/*.parquet\", format='parquet')"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Dividir los valores por salto de linea\r\n",
					"\r\n",
					"df_split = DataCruda.withColumn(\"contenido\", split(DataCruda[\"contenido\"], \"\\n\"))\r\n",
					"\r\n",
					"# Transformar la columna array para crear una nueva fila por cada elemento\r\n",
					"df_formated = df_split.select(df_split.index, df_split.pagenumber, df_split.x0, df_split.x1, df_split.y0, df_split.y1, explode(\"contenido\").alias(\"contenido\"))\r\n",
					"\r\n",
					"#Crear identificador unico  basado enel orden de lectura\r\n",
					"\r\n",
					"Incremental = Window.orderBy(\"pagenumber\",\"index\")\r\n",
					"df_formated = df_formated.withColumn(\"UniqueId\", row_number().over(Incremental))"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df_formated)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Obtener Informacion de Cliente\r\n",
					"\r\n",
					"Nombre = df_formated.select(df_formated.contenido).where((df_formated.UniqueId == 1)).first()[0]\r\n",
					"Email = user\r\n",
					"Direccion = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 3).first()[0]\r\n",
					"\r\n",
					"CiudadData = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 4).first()[0]\r\n",
					"CiudadData = CiudadData.replace(\"$\",\"\")\r\n",
					"CiudadData = CiudadData.replace(\"D.C.\",\"\")\r\n",
					"CiudadData = CiudadData.replace(\"  \",\" \")\r\n",
					"CiudadData = CiudadData.split(\" \")\r\n",
					"\r\n",
					"CiudadData[0] = CiudadData[0].replace(\"BOGOTA\",\"BOGOTA D.C\")\r\n",
					"Ciudad_1 = CiudadData[0].strip()\r\n",
					"\r\n",
					"CiudadData[1] = CiudadData[1].replace(\"BOGOTA\",\"BOGOTA D.C\")\r\n",
					"Ciudad_2 = CiudadData[1].strip()\r\n",
					"\r\n",
					"# Crear dataframe para informacion de cliente\r\n",
					"\r\n",
					"schemaCliente = StructType([ \\\r\n",
					"    StructField(\"Nombre\",StringType(),True), \\\r\n",
					"    StructField(\"Email\",StringType(),True), \\\r\n",
					"    StructField(\"Direccion\",StringType(),True), \\\r\n",
					"    StructField(\"Ciudad_1\",StringType(),True), \\\r\n",
					"    StructField(\"Ciudad_2\", StringType(), True), \\\r\n",
					"    StructField(\"File_name\", StringType(), True) \\\r\n",
					"  ])\r\n",
					"\r\n",
					"data_Cliente = [(Nombre, Email, Direccion, Ciudad_1, Ciudad_2, name)]\r\n",
					"\r\n",
					"df_Cliente = spark.createDataFrame(data=data_Cliente,schema=schemaCliente)\r\n",
					"\r\n",
					""
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Obtener Informacion de Prodcuto\r\n",
					"\r\n",
					"TipoProducto = 'Fiducuenta'\r\n",
					"NumeroProducto = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 19).first()[0]\r\n",
					"Desde = df_formated.select(regexp_replace(df_formated.contenido, '\\D', '')).where(df_formated.UniqueId == 12).first()[0]\r\n",
					"Hasta = df_formated.select(regexp_replace(df_formated.contenido, '\\D', '')).where(df_formated.UniqueId == 20).first()[0]\r\n",
					"\r\n",
					"ValorUnidad = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 21).first()[0]\r\n",
					"ValorUnidad = ValorUnidad.replace(\".\",\"\") \r\n",
					"ValorUnidad = ValorUnidad.replace(\",\",\".\") \r\n",
					"\r\n",
					"Rentabilidad = df_formated.select(regexp_replace(df_formated.contenido, '[aA-zZ]|%|\\s', '')).where(df_formated.UniqueId == 22).first()[0]\r\n",
					"if \"-\" in Rentabilidad: Rentabilidad = \"-\"+Rentabilidad.replace(\"-\",\"\")\r\n",
					"Rentabilidad = Rentabilidad.replace(\",\",\".\") \r\n",
					"\r\n",
					"Comision = df_formated.select(regexp_replace(df_formated.contenido, '[aA-zZ]|%|\\s', '')).where(df_formated.UniqueId == 17).first()[0]\r\n",
					"Comision = Comision.replace(\",\",\".\") \r\n",
					"\r\n",
					"# Crear dataframe para informacion de producto\r\n",
					"\r\n",
					"schemaProducto = StructType([ \\\r\n",
					"    StructField(\"TipoProducto\",StringType(),True), \\\r\n",
					"    StructField(\"NumeroProducto\",StringType(),True), \\\r\n",
					"    StructField(\"Desde\",StringType(),True), \\\r\n",
					"    StructField(\"Hasta\",StringType(),True), \\\r\n",
					"    StructField(\"ValorUnidad\",StringType(),True), \\\r\n",
					"    StructField(\"Rentabilidad\",StringType(),True), \\\r\n",
					"    StructField(\"Comision\",StringType(),True), \\\r\n",
					"    StructField(\"File_name\", StringType(), True)\r\n",
					"  ])\r\n",
					"\r\n",
					"data_Producto = [(TipoProducto, NumeroProducto, Desde, Hasta, ValorUnidad, Rentabilidad, Comision, name)]\r\n",
					"\r\n",
					"df_Producto = spark.createDataFrame(data=data_Producto,schema=schemaProducto)\r\n",
					""
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Extraer informacion de transacciones\r\n",
					"\r\n",
					"# Definir incrementales basado en el uniqueid\r\n",
					"\r\n",
					"Incremental = Window.orderBy(\"UniqueId\")\r\n",
					"\r\n",
					"#Extraer Fecha de transacciones\r\n",
					"\r\n",
					"df_Fecha = df_formated.select(regexp_replace(df_formated.contenido, '[aA-zZ]|%|\\s', '').alias(\"FechaTransaccion\"), df_formated.UniqueId).where \\\r\n",
					"    ( \\\r\n",
					"        ((df_formated.x0 >= 27.0) & (df_formated.x0 <= 28.0)) & \\\r\n",
					"        (df_formated.contenido != \"\")\r\n",
					"    )\r\n",
					"df_Fecha = df_Fecha.withColumn(\"id\", row_number().over(Incremental))\r\n",
					"\r\n",
					"# Obtener cantidad de transacciones\r\n",
					"CantidadTransacciones = df_Fecha.count()\r\n",
					"\r\n",
					"#Extraer Descripcion de transacciones\r\n",
					"\r\n",
					"df_Descripcion = df_formated.select(regexp_replace(df_formated.contenido, '\\d', '').alias(\"Descripcion\"), df_formated.UniqueId).where \\\r\n",
					"    ( \\\r\n",
					"        ((df_formated.x0 >= 27.0) & (df_formated.x0 <= 28.0)) & \\\r\n",
					"        (df_formated.contenido != \"\")\r\n",
					"    )\r\n",
					"df_Descripcion = df_Descripcion.withColumn(\"id\", row_number().over(Incremental))\r\n",
					"\r\n",
					"\r\n",
					"#Extraer Valor en $\r\n",
					"\r\n",
					"df_ValorPesos = df_formated.select(df_formated.contenido.alias(\"ValorPesos\"), df_formated.UniqueId).where \\\r\n",
					"    ( \\\r\n",
					"        ((df_formated.x0 >= 343.0) & (df_formated.x0 <= 344.0)) & \\\r\n",
					"        (df_formated.contenido != \"\")\r\n",
					"    )\r\n",
					"df_ValorPesos = df_ValorPesos.withColumn(\"ValorPesos\", regexp_replace(df_ValorPesos.ValorPesos, '[.]', ''))  \r\n",
					"df_ValorPesos = df_ValorPesos.withColumn(\"ValorPesos\", regexp_replace(df_ValorPesos.ValorPesos, ',', '.'))\r\n",
					"df_ValorPesos = df_ValorPesos.withColumn(\"id\", row_number().over(Incremental))\r\n",
					"\r\n",
					"\r\n",
					"#Extraer Valor en unidades\r\n",
					"\r\n",
					"df_ValorUnidades = df_formated.select(df_formated.contenido.alias(\"ValorUnidades\"), df_formated.UniqueId).where \\\r\n",
					"    ( \\\r\n",
					"        ((df_formated.x1 >= 501.5) & (df_formated.x1 <= 502.5)) & \\\r\n",
					"        (df_formated.contenido != \"\")\r\n",
					"    )\r\n",
					"df_ValorUnidades = df_ValorUnidades.withColumn(\"ValorUnidades\", regexp_replace(df_ValorUnidades.ValorUnidades, '[.]', ''))  \r\n",
					"df_ValorUnidades = df_ValorUnidades.withColumn(\"ValorUnidades\", regexp_replace(df_ValorUnidades.ValorUnidades, ',', '.'))\r\n",
					"df_ValorUnidades = df_ValorUnidades.withColumn(\"id\", row_number().over(Incremental))\r\n",
					"\r\n",
					"\r\n",
					"#Extraer Saldo\r\n",
					"\r\n",
					"df_Saldo = df_formated.select(df_formated.contenido.alias(\"Saldo\"), df_formated.UniqueId).where \\\r\n",
					"    ( \\\r\n",
					"        ((df_formated.x1 >= 593.5) & (df_formated.x1 <= 594.5)) & \\\r\n",
					"        (df_formated.contenido != \"\")\r\n",
					"    )\r\n",
					"df_Saldo = df_Saldo.withColumn(\"Saldo\", regexp_replace(df_Saldo.Saldo, '[.]', ''))  \r\n",
					"df_Saldo = df_Saldo.withColumn(\"Saldo\", regexp_replace(df_Saldo.Saldo, ',', '.'))\r\n",
					"df_Saldo = df_Saldo.withColumn(\"id\", row_number().over(Incremental))\r\n",
					"\r\n",
					"\r\n",
					"#Agregar pdf name \r\n",
					"\r\n",
					"schema_pdf_name = StructType([\r\n",
					"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"pdfname\", StringType(), nullable=True)    \r\n",
					"])\r\n",
					"\r\n",
					"df_file_name = spark.createDataFrame([], schema_pdf_name)\r\n",
					"\r\n",
					"\r\n",
					"schema_email = StructType([\r\n",
					"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"email\", StringType(), nullable=True)    \r\n",
					"])\r\n",
					"\r\n",
					"df_email = spark.createDataFrame([], schema_email)\r\n",
					"\r\n",
					"schema_producto = StructType([\r\n",
					"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
					"    StructField(\"TipoProducto\", StringType(), nullable=True),\r\n",
					"    StructField(\"NumeroProducto\", StringType(), nullable=True)   \r\n",
					"])\r\n",
					"\r\n",
					"df_producto = spark.createDataFrame([], schema_producto)\r\n",
					"\r\n",
					"for i in range (CantidadTransacciones):                \r\n",
					"    newRow = spark.createDataFrame([(i+1,name)])\r\n",
					"    df_file_name = df_file_name.union(newRow)\r\n",
					"\r\n",
					"    newRow = spark.createDataFrame([(i+1,user)])\r\n",
					"    df_email = df_email.union(newRow)\r\n",
					"\r\n",
					"    newRow = spark.createDataFrame([(i+1,TipoProducto,NumeroProducto)])\r\n",
					"    df_producto = df_producto.union(newRow)\r\n",
					""
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#df_Fecha.show()\r\n",
					"#df_Descripcion.show()\r\n",
					"#df_ValorPesos.show()\r\n",
					"#df_ValorUnidades.show()\r\n",
					"#df_Saldo.show()"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"df_Transacciones = df_Fecha.join(df_Descripcion, df_Fecha.id == df_Descripcion.id, \"inner\") \\\r\n",
					"                  .join(df_ValorPesos , df_Fecha.id == df_ValorPesos.id, \"inner\") \\\r\n",
					"                  .join(df_ValorUnidades, df_Fecha.id == df_ValorUnidades.id, \"inner\") \\\r\n",
					"                  .join(df_Saldo, df_Fecha.id == df_Saldo.id, \"inner\") \\\r\n",
					"                  .join(df_file_name, df_Fecha.id == df_file_name.id, \"inner\") \\\r\n",
					"                  .join(df_email, df_Fecha.id == df_email.id, \"inner\") \\\r\n",
					"                  .join(df_producto, df_Fecha.id == df_producto.id, \"inner\") \\\r\n",
					"                  .select( \\\r\n",
					"                        df_Fecha.FechaTransaccion, \\\r\n",
					"                        df_Descripcion.Descripcion, \\\r\n",
					"                        df_ValorPesos.ValorPesos, \\\r\n",
					"                        df_ValorUnidades.ValorUnidades, \\\r\n",
					"                        df_Saldo.Saldo, \\\r\n",
					"                        df_file_name.pdfname, \\\r\n",
					"                        df_email.email, \\\r\n",
					"                        df_producto.TipoProducto, \\\r\n",
					"                        df_producto.NumeroProducto \\\r\n",
					"                        )"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"display(df_Cliente)\r\n",
					"display(df_Producto)\r\n",
					"#display(df_Transacciones)"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#Generar archivos con informacion\r\n",
					"\r\n",
					"Routeadl = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net\"\r\n",
					"path_sink_comp = fecha+\"/\"+user+\"/\"+name\r\n",
					"\r\n",
					"# crear parquet file para cliente\r\n",
					"df_Cliente.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Cliente/\"+path_sink_comp)\r\n",
					"\r\n",
					"# crear parquet file para porducto\r\n",
					"df_Producto.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Producto/\"+path_sink_comp)\r\n",
					"\r\n",
					"# crear parquet file para transacciones\r\n",
					"df_Transacciones.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Transacciones/\"+path_sink_comp)\r\n",
					""
				],
				"execution_count": 78
			}
		]
	}
}