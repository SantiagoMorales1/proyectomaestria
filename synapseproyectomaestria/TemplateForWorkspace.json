{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Nombre del área de trabajo",
			"defaultValue": "synapseproyectomaestria"
		},
		"synapseproyectomaestria-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Cadena protegida para \"connectionString\"de \"synapseproyectomaestria-WorkspaceDefaultSqlServer\"",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapseproyectomaestria.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synapseproyectomaestria-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalakeproyectomaestria.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline - Process PDF Files - Stage Zone')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Reading PDF Files - Stagefiles Zone",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Procesar Archivos PDF - Zona Stage",
								"type": "NotebookReference"
							},
							"parameters": {
								"pdf_user": {
									"value": "bsmoralesg@outlook.com",
									"type": "string"
								},
								"pdf_type": {
									"value": "Tarjeta Credito",
									"type": "string"
								},
								"pdf_fecha": {
									"value": "10012023",
									"type": "string"
								},
								"file_name": {
									"value": "Extracto_7358651_202108_TARJETA_VISA_2166.pdf",
									"type": "string"
								},
								"pdf_password": {
									"value": "1233491047",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "spproymaestria",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 1
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					},
					{
						"name": "Reading Raw Data - StageData Zone",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Reading PDF Files - Stagefiles Zone",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Procesar Data Cruda PDF - Zona Data - Tarjeta Credito",
								"type": "NotebookReference"
							},
							"parameters": {
								"user": {
									"value": "bsmoralesg@outlook.com",
									"type": "string"
								},
								"type": {
									"value": "Tarjeta Credito",
									"type": "string"
								},
								"fecha": {
									"value": "10012023",
									"type": "string"
								},
								"name": {
									"value": "Extracto_7358651_202108_TARJETA_VISA_2166.pdf",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "spproymaestria",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 1
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Procesar Archivos PDF - Zona Stage')]",
				"[concat(variables('workspaceId'), '/bigDataPools/spproymaestria')]",
				"[concat(variables('workspaceId'), '/notebooks/Procesar Data Cruda PDF - Zona Data - Tarjeta Credito')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLakeMaster')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapseproyectomaestria-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "stagedata"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapseproyectomaestria-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapseproyectomaestria-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapseproyectomaestria-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapseproyectomaestria-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapseproyectomaestria-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_CuentaAhorros_Cliente')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.CuentaAhorros_Cliente (\n\t[Nombre] nvarchar(4000),\n\t[Email] nvarchar(4000),\n\t[Direccion] nvarchar(4000),\n\t[Ciudad_1] nvarchar(4000),\n\t[Ciudad_2] nvarchar(4000),\n\t[File_name] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Cuenta Ahorros/Cliente/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_CuentaAhorros_Producto')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.CuentaAhorros_Producto (\n\t[TipoProducto] nvarchar(4000),\n\t[NumeroProducto] nvarchar(4000),\n\t[Sucursal] nvarchar(4000),\n\t[File_name] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Cuenta Ahorros/Producto/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_CuentaAhorros_Transacciones')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.CuentaAhorros_Transacciones (\n\t[FechaTransaccion] nvarchar(4000),\n\t[DescripcionTransaccion] nvarchar(4000),\n\t[Sucursal] nvarchar(4000),\n\t[Descuento] nvarchar(4000),\n\t[ValorTransaccion] nvarchar(4000),\n\t[Saldo] nvarchar(4000),\n\t[pdfname] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Cuenta Ahorros/Transacciones/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_FondoInversion_Cliente')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.FondoInversion_Cliente (\n\t[Nombre] nvarchar(4000),\n\t[Email] nvarchar(4000),\n\t[Direccion] nvarchar(4000),\n\t[Ciudad_1] nvarchar(4000),\n\t[Ciudad_2] nvarchar(4000),\n\t[File_name] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Fondos de Inversion/Cliente/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_FondoInversion_Producto')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.FondoInversion_Producto (\n\t[TipoProducto] nvarchar(4000),\n\t[NumeroProducto] nvarchar(4000),\n\t[Desde] nvarchar(4000),\n\t[Hasta] nvarchar(4000),\n\t[ValorUnidad] nvarchar(4000),\n\t[Rentabilidad] nvarchar(4000),\n\t[Comision] nvarchar(4000),\n\t[File_name] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Fondos de Inversion/Producto/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_FondoInversion_Transacciones')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.FondoInversion_Transacciones (\n\t[FechaTransaccion] nvarchar(4000),\n\t[Descripcion] nvarchar(4000),\n\t[ValorPesos] nvarchar(4000),\n\t[ValorUnidades] nvarchar(4000),\n\t[Saldo] nvarchar(4000),\n\t[pdfname] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Fondos de Inversion/Transacciones/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_TarjetaCredito_Cliente')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.TarjetaCredito_Cliente (\n\t[Nombre] nvarchar(4000),\n\t[Email] nvarchar(4000),\n\t[Direccion] nvarchar(4000),\n\t[Ciudad_1] nvarchar(4000),\n\t[Ciudad_2] nvarchar(4000),\n\t[File_name] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Tarjeta Credito/Cliente/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_TarjetaCredito_Producto')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.TarjetaCredito_Producto (\n\t[TipoProducto] nvarchar(4000),\n\t[CupoTotal] nvarchar(4000),\n\t[NumeroTarjeta] nvarchar(4000),\n\t[TasaInteresMVCompra1Mes] nvarchar(4000),\n\t[TasaInteresMVCompra2MesOMas] nvarchar(4000),\n\t[TasaInteresMVImpuestos] nvarchar(4000),\n\t[TasaInteresMVAvances] nvarchar(4000),\n\t[TasaInteresMVMora] nvarchar(4000),\n\t[TasaInteresEACompra1Mes] nvarchar(4000),\n\t[TasaInteresEACompra2MesOMas] nvarchar(4000),\n\t[TasaInteresEAImpuestos] nvarchar(4000),\n\t[TasaInteresEAAvances] nvarchar(4000),\n\t[TasaInteresEAMora] nvarchar(4000),\n\t[File_name] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Tarjeta Credito/Producto/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM stage.TarjetaCredito_Producto\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ET_TarjetaCredito_Transacciones')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "External Tables"
				},
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseParquetFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseParquetFormat] \n\tWITH ( FORMAT_TYPE = PARQUET)\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net' \n\t)\nGO\n\nCREATE EXTERNAL TABLE stage.TarjetaCredito_Transacciones (\n\t[IdTransaccion] nvarchar(4000),\n\t[FechaTransaccion] nvarchar(4000),\n\t[DescripcionTransaccion] nvarchar(4000),\n\t[CostoTransaccion] nvarchar(4000),\n\t[TasaPactada] nvarchar(4000),\n\t[TasaFacturada] nvarchar(4000),\n\t[CargosYAbonos] nvarchar(4000),\n\t[SaldoADiferir] nvarchar(4000),\n\t[Cuotas] nvarchar(4000),\n\t[pdfname] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'Tarjeta Credito/Transacciones/',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseParquetFormat]\n\t)\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SCH_params')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Schemas"
				},
				"content": {
					"query": "create schema params\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SCH_stage')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Schemas"
				},
				"content": {
					"query": "create schema stage",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TBL_params_datausers')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n--drop table params.datausers \nCreate table params.datausers (\n    id bigint identity (1, 1),\n    usuario varchar(150) not null,\n    fecha varchar(12)  not null,\n    type varchar(20)  not null,\n    pdfname varchar(150)  not null,\n    password varchar(150) null,\n    fecha_registro datetime  not null\n)\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Procesar Archivos PDF - Zona Stage')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "LecturaScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "fa0fb876-25a0-4acf-b853-b93493afc0d3"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#Importando Librerias\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from io import BytesIO\r\n",
							"from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\r\n",
							"from pdfminer.pdfpage import PDFPage\r\n",
							"from pdfminer.converter import TextConverter\r\n",
							"from pdfminer.layout import LAParams, LTTextBoxHorizontal, LTFigure\r\n",
							"from pdfminer.converter import PDFPageAggregator\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\r\n",
							"import io"
						],
						"outputs": [],
						"execution_count": 137
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Creando sesion \r\n",
							"\r\n",
							"spark = SparkSession.builder \\\r\n",
							"    .appName(\"PDF Extraction\") \\\r\n",
							"    .getOrCreate()"
						],
						"outputs": [],
						"execution_count": 138
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"#Capturar Parametros\r\n",
							"pdf_user = \"bsmoralesg@outlook.com\"\r\n",
							"pdf_type = \"Tarjeta Credito\"\r\n",
							"#pdf_type = \"Cuenta Ahorros\"\r\n",
							"#pdf_type = \"Fondos de Inversion\"\r\n",
							"pdf_fecha = \"10012023\"\r\n",
							"\r\n",
							"#Tarjeta Credito\r\n",
							"#file_name = \"Extracto_403340132_202305_TARJETA_VISA_2166.pdf\"\r\n",
							"#file_name = \"Extracto_7358651_202108_TARJETA_VISA_2166.pdf\"\r\n",
							"#file_name = \"Extracto_392576648_202304_TARJETA_VISA_2166.pdf\"\r\n",
							"#file_name = \"Extracto_369280261_202303_TARJETA_VISA_2166.pdf\"\r\n",
							"#file_name = \"Extracto_231339919_202209_TARJETA_VISA_2166.pdf\"\r\n",
							"#file_name = \"Extracto_204864725_202207_TARJETA_VISA_2166.pdf\"\r\n",
							"\r\n",
							"#Cuenta de Ahorrros\r\n",
							"\r\n",
							"#file_name = \"Extracto_49243429_202112_CTA_AHORROS_5557.pdf\"\r\n",
							"#file_name = \"Extracto_240056999_202209_CTA_AHORROS_5557.pdf\"\r\n",
							"#file_name = \"Extracto_307612445_202212_CTA_AHORROS_5557.pdf\"\r\n",
							"#file_name = \"Extracto_375992443_202303_CTA_AHORROS_5557.pdf\"\r\n",
							"\r\n",
							"#Fiducuenta\r\n",
							"\r\n",
							"#file_name = \"Extracto_8791222_202108_FIDUCUENTA_3940.pdf\"\r\n",
							"#file_name = \"Extracto_36546412_202111_FIDUCUENTA_3940.pdf\"\r\n",
							"#file_name = \"Extracto_212010590_202208_FIDUCUENTA_3940.pdf\"\r\n",
							"#file_name = \"Extracto_254154275_202210_FIDUCUENTA_3940.pdf\"\r\n",
							"#file_name = \"Extracto_333733883_202301_FIDUCUENTA_3940.pdf\"\r\n",
							"#file_name = \"Extracto_395097292_202304_FIDUCUENTA_3940.pdf\"\r\n",
							"\r\n",
							"\r\n",
							"pdf_password = \"1233491047\""
						],
						"outputs": [],
						"execution_count": 139
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Consolidar Variables\r\n",
							"\r\n",
							"#Ruta Origen\r\n",
							"pdf_path_origin = \"abfss://stagefiles@datalakeproyectomaestria.dfs.core.windows.net/\"+pdf_type+\"/\"+pdf_fecha+\"/\"+pdf_user+\"/\"+file_name\r\n",
							"\r\n",
							"#Ruta Destino\r\n",
							"file_name = file_name.replace(\".pdf\", \"\")\r\n",
							"path_sink = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net/\"+pdf_type+\"/Raw/\"+pdf_fecha+\"/\"+pdf_user+\"/\"+file_name+\"/\""
						],
						"outputs": [],
						"execution_count": 140
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Definir estructura para datos crudos\r\n",
							"\r\n",
							"schemarawdata = StructType([\r\n",
							"    StructField(\"index\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"pagenumber\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"x0\", DoubleType(), nullable=True),\r\n",
							"    StructField(\"x1\", DoubleType(), nullable=True),\r\n",
							"    StructField(\"y0\", DoubleType(), nullable=True),\r\n",
							"    StructField(\"y1\", DoubleType(), nullable=True),\r\n",
							"    StructField(\"contenido\", StringType(), nullable=True)\r\n",
							"])\r\n",
							"\r\n",
							"df_RawDat = spark.createDataFrame([], schemarawdata)\r\n",
							"ArrayRawData = []\r\n",
							"\r\n",
							" "
						],
						"outputs": [],
						"execution_count": 141
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Procesar informacion - Capturar informacion\r\n",
							"\r\n",
							"\r\n",
							"# Leer the PDF file como bytes\r\n",
							"pdf_bytes = spark.read \\\r\n",
							"    .format(\"binaryFile\") \\\r\n",
							"    .option(\"path\", pdf_path_origin) \\\r\n",
							"    .load() \\\r\n",
							"    .select(\"content\") \\\r\n",
							"    .collect()[0][0]\r\n",
							"\r\n",
							"# crear un BytesIO stream de the PDF bytes\r\n",
							"pdf_stream = BytesIO(pdf_bytes)\r\n",
							"pdf_stream.seek(0)\r\n",
							"\r\n",
							"# Crear objeto PDFResourceManager y configurar parametros\r\n",
							"rsrcmgr = PDFResourceManager()\r\n",
							"retstr = io.StringIO()\r\n",
							"\r\n",
							"# Crear Objeto  TextConverter para las paginas dle PDF\r\n",
							"laparams = LAParams()\r\n",
							"device = TextConverter(rsrcmgr, retstr, laparams=laparams)\r\n",
							"\r\n",
							"# Crear objeto PDFPageInterpreter\r\n",
							"interpreter = PDFPageInterpreter(rsrcmgr, device)\r\n",
							"\r\n",
							"# Porcesar cada pagina del archivo PDF\r\n",
							"password = pdf_password.encode()\r\n",
							"pages = PDFPage.get_pages(pdf_stream, password=password, check_extractable=True)\r\n",
							"\r\n",
							"for page in pages:\r\n",
							"    interpreter.process_page(page)\r\n",
							"\r\n",
							"# Obtener el texto extraido del objeto StringIO\r\n",
							"pdf_content = retstr.getvalue()\r\n",
							"\r\n",
							"# Cerrar el objeto StringIO\r\n",
							"device.close()\r\n",
							"retstr.close()"
						],
						"outputs": [],
						"execution_count": 142
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Leer the PDF file como bytes\r\n",
							"pdf_bytes = spark.read \\\r\n",
							"    .format(\"binaryFile\") \\\r\n",
							"    .option(\"path\", pdf_path_origin) \\\r\n",
							"    .load() \\\r\n",
							"    .select(\"content\") \\\r\n",
							"    .collect()[0][0]\r\n",
							"\r\n",
							"# crear un BytesIO stream de the PDF bytes\r\n",
							"pdf_stream = BytesIO(pdf_bytes)\r\n",
							"pdf_stream.seek(0)\r\n",
							"\r\n",
							"# Crear objeto PDFResourceManager y configurar parametros\r\n",
							"rsrcmgr = PDFResourceManager()\r\n",
							"retstr = io.StringIO()\r\n",
							"\r\n",
							"# Crear Objeto  TextConverter para las paginas dle PDF\r\n",
							"laparams = LAParams()\r\n",
							"#device = TextConverter(rsrcmgr, retstr, laparams=laparams)\r\n",
							"device = PDFPageAggregator(rsrcmgr, laparams=laparams)\r\n",
							"\r\n",
							"# Crear objeto PDFPageInterpreter\r\n",
							"interpreter = PDFPageInterpreter(rsrcmgr, device)\r\n",
							"\r\n",
							"# Porcesar cada pagina del archivo PDF\r\n",
							"password = pdf_password.encode()\r\n",
							"pages = PDFPage.get_pages(pdf_stream, password=password, check_extractable=True)    \r\n",
							"\r\n",
							"\r\n",
							"#Recorrer las Paginas del PDF \r\n",
							"pageraux = None\r\n",
							"pagenumber = 0\r\n",
							"\r\n",
							"for page in pages:\r\n",
							"        \r\n",
							"        # Process the page\r\n",
							"\r\n",
							"        interpreter.process_page(page)\r\n",
							"        layout = device.get_result()\r\n",
							"\r\n",
							"        # Cambiar la pagina\r\n",
							"        if (page != pageraux):\r\n",
							"            pagenumber = pagenumber + 1\r\n",
							"        \r\n",
							"        # Iterar por cada elemnto del layout        \r\n",
							"        for element in layout:\r\n",
							"            if (not isinstance(element, LTFigure)):\r\n",
							"                \r\n",
							"                index = int(element.index)                \r\n",
							"                x0 = float(element.x0)\r\n",
							"                x1 = float(element.x1)\r\n",
							"                y0 = float(element.y0)\r\n",
							"                y1 = float(element.y1)\r\n",
							"                contenido = str(element.get_text())\r\n",
							"                ArrayRawData.append((index, pagenumber, x0, x1, y0, y1, contenido)) \r\n",
							"    \r\n",
							"        pageraux = page\r\n",
							""
						],
						"outputs": [],
						"execution_count": 143
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Crear dataframe con toda la data cruda\r\n",
							"\r\n",
							"df_RawDat = spark.createDataFrame(ArrayRawData, schemarawdata)\r\n",
							"\r\n",
							"# crear parquet file\r\n",
							"df_RawDat.write.mode(\"overwrite\").parquet(path_sink)"
						],
						"outputs": [],
						"execution_count": 144
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_RawDat)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Procesar Data Cruda PDF - Zona Data - Cuenta Ahorro')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "LecturaScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "8e032ec0-adf3-464b-b869-01bed8c3c0e8"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"#Importar Librerias Necesarias\r\n",
							"\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql.functions import *"
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Configurar parametros aqui\r\n",
							"\r\n",
							"user = \"bsmoralesg@outlook.com\"\r\n",
							"type = \"Cuenta Ahorros\"\r\n",
							"fecha = \"10012023\"\r\n",
							"\r\n",
							"#name = \"Extracto_49243429_202112_CTA_AHORROS_5557\"\r\n",
							"#name = \"Extracto_240056999_202209_CTA_AHORROS_5557\"\r\n",
							"#name = \"Extracto_307612445_202212_CTA_AHORROS_5557\"\r\n",
							"name = \"Extracto_375992443_202303_CTA_AHORROS_5557\""
						],
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Configurar rutas de origen y destino de archivos\r\n",
							"\r\n",
							"pathsource = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net/\"+type+\"/Raw/\"+fecha+\"/\"+user+\"/\"+name"
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Obtener Datos para Tarjeta Credito\r\n",
							"\r\n",
							"DataCruda = spark.read.load(pathsource+\"/*.parquet\", format='parquet')"
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Dividir los valores por salto de linea\r\n",
							"\r\n",
							"df_split = DataCruda.withColumn(\"contenido\", split(DataCruda[\"contenido\"], \"\\n\"))\r\n",
							"\r\n",
							"# Transformar la columna array para crear una nueva fila por cada elemento\r\n",
							"df_formated = df_split.select(df_split.index, df_split.pagenumber, df_split.x0, df_split.x1, df_split.y0, df_split.y1, explode(\"contenido\").alias(\"contenido\"))\r\n",
							"\r\n",
							"#Crear identificador unico  basado enel orden de lectura\r\n",
							"\r\n",
							"Incremental = Window.orderBy(\"pagenumber\",\"index\")\r\n",
							"df_formated = df_formated.withColumn(\"UniqueId\", row_number().over(Incremental))"
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display (df_formated)"
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Obtener Informacion de Cliente\r\n",
							"\r\n",
							"Nombre = df_formated.select(df_formated.contenido).where((df_formated.UniqueId == 1)).first()[0]\r\n",
							"Email = user\r\n",
							"Direccion = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 3).first()[0]\r\n",
							"\r\n",
							"CiudadData = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 5).first()[0]\r\n",
							"CiudadData = CiudadData.replace(\"$\",\"\")\r\n",
							"CiudadData = CiudadData.replace(\"D.C.\",\"\")\r\n",
							"CiudadData = CiudadData.split(\"  \")\r\n",
							"\r\n",
							"CiudadData[0] = CiudadData[0].replace(\"BOGOTA\",\"BOGOTA D.C\")\r\n",
							"Ciudad_1 = CiudadData[0].strip()\r\n",
							"\r\n",
							"CiudadData[1] = CiudadData[1].replace(\"BOGOTA\",\"BOGOTA D.C\")\r\n",
							"Ciudad_2 = CiudadData[1].strip()\r\n",
							"\r\n",
							"# Crear dataframe para informacion de cliente\r\n",
							"\r\n",
							"schemaCliente = StructType([ \\\r\n",
							"    StructField(\"Nombre\",StringType(),True), \\\r\n",
							"    StructField(\"Email\",StringType(),True), \\\r\n",
							"    StructField(\"Direccion\",StringType(),True), \\\r\n",
							"    StructField(\"Ciudad_1\",StringType(),True), \\\r\n",
							"    StructField(\"Ciudad_2\", StringType(), True), \\\r\n",
							"    StructField(\"File_name\", StringType(), True) \\\r\n",
							"  ])\r\n",
							"\r\n",
							"data_Cliente = [(Nombre, Email, Direccion, Ciudad_1, Ciudad_2, name)]\r\n",
							"\r\n",
							"df_Cliente = spark.createDataFrame(data=data_Cliente,schema=schemaCliente)"
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Obtener Informacion de Prodcuto\r\n",
							"\r\n",
							"TipoProducto = 'Cuenta Ahorros'\r\n",
							"NumeroProducto = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 16).first()[0]\r\n",
							"Sucursal = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 14).first()[0]\r\n",
							"\r\n",
							"# Crear dataframe para informacion de producto\r\n",
							"\r\n",
							"schemaProducto = StructType([ \\\r\n",
							"    StructField(\"TipoProducto\",StringType(),True), \\\r\n",
							"    StructField(\"NumeroProducto\",StringType(),True), \\\r\n",
							"    StructField(\"Sucursal\",StringType(),True), \\\r\n",
							"    StructField(\"File_name\", StringType(), True) \\\r\n",
							"  ])\r\n",
							"\r\n",
							"data_Producto = [(TipoProducto, NumeroProducto, Sucursal, name)]\r\n",
							"\r\n",
							"df_Producto = spark.createDataFrame(data=data_Producto,schema=schemaProducto)"
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Extraer informacion de transacciones\r\n",
							"\r\n",
							"# Definir incrementales basado en el uniqueid\r\n",
							"\r\n",
							"Incremental = Window.orderBy(\"UniqueId\")\r\n",
							"\r\n",
							"#Extraer Fecha de transacciones\r\n",
							"\r\n",
							"df_Fecha = df_formated.select(df_formated.contenido.alias(\"FechaTransaccion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x0 >= 31.0) & (df_formated.x0 <= 32.0)) & \\\r\n",
							"        (df_formated.contenido != \"FECHA\")  & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_Fecha = df_Fecha.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"# Obtener cantidad de transacciones\r\n",
							"CantidadTransacciones = df_Fecha.count()\r\n",
							"\r\n",
							"\r\n",
							"#Extraer descripcion transacciones\r\n",
							"\r\n",
							"df_Descripciones = df_formated.select(df_formated.contenido.alias(\"DescripcionTransaccion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x0 >= 78.5) & (df_formated.x0 <= 81.0)) & \\\r\n",
							"        (df_formated.contenido != \"DESCRIPCIÓN\")  & \\\r\n",
							"        (df_formated.contenido != \"FIN ESTADO DE CUENTA\")  & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_Descripciones = df_Descripciones.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"#Extraer sucursal\r\n",
							"\r\n",
							"df_Sucursal = df_formated.select(df_formated.contenido.alias(\"Sucursal\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        (df_formated.x0 == 269.516) & \\\r\n",
							"        (df_formated.contenido != \"SUCURSAL\")  & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"\r\n",
							"if (df_Sucursal.count() !=  CantidadTransacciones):\r\n",
							"    MaximaCantidadRegistros = df_Sucursal.count()\r\n",
							"    RegistrosPendintes = CantidadTransacciones - MaximaCantidadRegistros\r\n",
							"    MaxUniqueID = df_Sucursal.select(max(df_Sucursal.UniqueId)).first()[0]\r\n",
							"    \r\n",
							"    if MaxUniqueID is None:\r\n",
							"        MaxUniqueID = 1\r\n",
							"\r\n",
							"    for i in range (RegistrosPendintes):                \r\n",
							"        newRow = spark.createDataFrame([(\"\",str(MaxUniqueID+(i+1)))])\r\n",
							"        df_Sucursal = df_Sucursal.union(newRow)\r\n",
							"\r\n",
							"df_Sucursal = df_Sucursal.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"#Extraer descuento\r\n",
							"\r\n",
							"df_Descuento = df_formated.select(df_formated.contenido.alias(\"Descuento\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        (df_formated.x0 == 355.095) & \\\r\n",
							"        (df_formated.contenido != \"DCTO.\")  & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"\r\n",
							"if (df_Descuento.count() !=  CantidadTransacciones):\r\n",
							"    MaximaCantidadRegistros = df_Descuento.count()\r\n",
							"    RegistrosPendintes = CantidadTransacciones - MaximaCantidadRegistros\r\n",
							"    MaxUniqueID = df_Descuento.select(max(df_Descuento.UniqueId)).first()[0]\r\n",
							"    \r\n",
							"    if MaxUniqueID is None:\r\n",
							"        MaxUniqueID = 1\r\n",
							"\r\n",
							"    for i in range (RegistrosPendintes):                \r\n",
							"        newRow = spark.createDataFrame([(\"\",str(MaxUniqueID+(i+1)))])\r\n",
							"        df_Descuento = df_Descuento.union(newRow)\r\n",
							"\r\n",
							"df_Descuento = df_Descuento.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer Valor Transaccion\r\n",
							"\r\n",
							"df_ValorTransaccion = df_formated.select(df_formated.contenido.alias(\"ValorTransaccion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x1 >= 501.0) & (df_formated.x0 <= 502.0)) & \\\r\n",
							"        (df_formated.contenido.rlike(\"[0-9]\")) & \\\r\n",
							"        (~ df_formated.contenido.rlike(\"[a-z]\")) & \\\r\n",
							"        (~ df_formated.contenido.rlike(\"[A-Z]\")) & \\\r\n",
							"        (df_formated.contenido != \"VALOR\")  & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_ValorTransaccion = df_ValorTransaccion.withColumn(\"ValorTransaccion\", regexp_replace(df_ValorTransaccion.ValorTransaccion, ',', '')) \r\n",
							"df_ValorTransaccion = df_ValorTransaccion.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer Saldo\r\n",
							"\r\n",
							"df_Saldo = df_formated.select(df_formated.contenido.alias(\"Saldo\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        (((df_formated.x0 >= 533.0) & (df_formated.x0 <= 534.0)) | ((df_formated.x0 >= 549.0) & (df_formated.x0 <= 550.0)) | ((df_formated.x0 >= 539.0) & (df_formated.x0 <= 540.0))) & \\\r\n",
							"        (df_formated.contenido != \"SALDO\")  & \\\r\n",
							"        (df_formated.contenido.rlike(\"[0-9]\")) & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_Saldo = df_Saldo.withColumn(\"Saldo\", regexp_replace(df_Saldo.Saldo, ',', '')) \r\n",
							"df_Saldo = df_Saldo.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"#Agregar pdf name \r\n",
							"\r\n",
							"schema_pdf_name = StructType([\r\n",
							"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"pdfname\", StringType(), nullable=True)    \r\n",
							"])\r\n",
							"\r\n",
							"df_file_name = spark.createDataFrame([], schema_pdf_name)\r\n",
							"\r\n",
							"for i in range (CantidadTransacciones):                \r\n",
							"    newRow = spark.createDataFrame([(i+1,name)])\r\n",
							"    df_file_name = df_file_name.union(newRow) \r\n",
							""
						],
						"outputs": [],
						"execution_count": 94
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_Fecha.show(200)\r\n",
							"#df_Sucursal.show(200)\r\n",
							"#df_Descuento.show(200)\r\n",
							"#df_ValorTransaccion.show(200)\r\n",
							"#df_Saldo.show(200)\r\n",
							"#df_file_name.show(200)"
						],
						"outputs": [],
						"execution_count": 95
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_Transacciones = df_Fecha.join(df_Descripciones, df_Fecha.id == df_Descripciones.id, \"inner\") \\\r\n",
							"                  .join(df_Sucursal , df_Fecha.id == df_Sucursal.id, \"inner\") \\\r\n",
							"                  .join(df_Descuento, df_Fecha.id == df_Descuento.id, \"inner\") \\\r\n",
							"                  .join(df_ValorTransaccion, df_Fecha.id == df_ValorTransaccion.id, \"inner\") \\\r\n",
							"                  .join(df_Saldo, df_Fecha.id == df_Saldo.id, \"inner\") \\\r\n",
							"                  .join(df_file_name, df_Fecha.id == df_file_name.id, \"inner\") \\\r\n",
							"                  .select( \\\r\n",
							"                        df_Fecha.FechaTransaccion, \\\r\n",
							"                        df_Descripciones.DescripcionTransaccion, \\\r\n",
							"                        df_Sucursal.Sucursal, \\\r\n",
							"                        df_Descuento.Descuento, \\\r\n",
							"                        df_ValorTransaccion.ValorTransaccion, \\\r\n",
							"                        df_Saldo.Saldo, \\\r\n",
							"                        df_file_name.pdfname\r\n",
							"                        )"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#display(df_Cliente)\r\n",
							"#display(df_Producto)\r\n",
							"#display(df_Transacciones)"
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Generar archivos con informacion\r\n",
							"\r\n",
							"Routeadl = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net\"\r\n",
							"path_sink_comp = fecha+\"/\"+user+\"/\"+name\r\n",
							"\r\n",
							"# crear parquet file para cliente\r\n",
							"df_Cliente.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Cliente/\"+path_sink_comp)\r\n",
							"\r\n",
							"# crear parquet file para porducto\r\n",
							"df_Producto.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Producto/\"+path_sink_comp)\r\n",
							"\r\n",
							"# crear parquet file para transacciones\r\n",
							"df_Transacciones.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Transacciones/\"+path_sink_comp)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 98
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Procesar Data Cruda PDF - Zona Data - Fondo de Inversion')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "LecturaScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "17740c57-7b59-41bd-8c44-046a0453e21c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"#Importar Librerias Necesarias\r\n",
							"\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql.functions import *"
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Configurar parametros aqui\r\n",
							"\r\n",
							"user = \"bsmoralesg@outlook.com\"\r\n",
							"type = \"Fondos de Inversion\"\r\n",
							"fecha = \"10012023\"\r\n",
							"\r\n",
							"#name = \"Extracto_8791222_202108_FIDUCUENTA_3940\"\r\n",
							"#name = \"Extracto_36546412_202111_FIDUCUENTA_3940\"\r\n",
							"#name = \"Extracto_212010590_202208_FIDUCUENTA_3940\"\r\n",
							"#name = \"Extracto_254154275_202210_FIDUCUENTA_3940\"\r\n",
							"#name = \"Extracto_333733883_202301_FIDUCUENTA_3940\"\r\n",
							"name = \"Extracto_395097292_202304_FIDUCUENTA_3940\"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Configurar rutas de origen de archivos\r\n",
							"\r\n",
							"pathsource = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net/\"+type+\"/Raw/\"+fecha+\"/\"+user+\"/\"+name"
						],
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Obtener Datos para Fondo de inversion\r\n",
							"\r\n",
							"DataCruda = spark.read.load(pathsource+\"/*.parquet\", format='parquet')"
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Dividir los valores por salto de linea\r\n",
							"\r\n",
							"df_split = DataCruda.withColumn(\"contenido\", split(DataCruda[\"contenido\"], \"\\n\"))\r\n",
							"\r\n",
							"# Transformar la columna array para crear una nueva fila por cada elemento\r\n",
							"df_formated = df_split.select(df_split.index, df_split.pagenumber, df_split.x0, df_split.x1, df_split.y0, df_split.y1, explode(\"contenido\").alias(\"contenido\"))\r\n",
							"\r\n",
							"#Crear identificador unico  basado enel orden de lectura\r\n",
							"\r\n",
							"Incremental = Window.orderBy(\"pagenumber\",\"index\")\r\n",
							"df_formated = df_formated.withColumn(\"UniqueId\", row_number().over(Incremental))"
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display(df_formated)"
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Obtener Informacion de Cliente\r\n",
							"\r\n",
							"Nombre = df_formated.select(df_formated.contenido).where((df_formated.UniqueId == 1)).first()[0]\r\n",
							"Email = user\r\n",
							"Direccion = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 3).first()[0]\r\n",
							"\r\n",
							"CiudadData = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 4).first()[0]\r\n",
							"CiudadData = CiudadData.replace(\"$\",\"\")\r\n",
							"CiudadData = CiudadData.replace(\"D.C.\",\"\")\r\n",
							"CiudadData = CiudadData.replace(\"  \",\" \")\r\n",
							"CiudadData = CiudadData.split(\" \")\r\n",
							"\r\n",
							"CiudadData[0] = CiudadData[0].replace(\"BOGOTA\",\"BOGOTA D.C\")\r\n",
							"Ciudad_1 = CiudadData[0].strip()\r\n",
							"\r\n",
							"CiudadData[1] = CiudadData[1].replace(\"BOGOTA\",\"BOGOTA D.C\")\r\n",
							"Ciudad_2 = CiudadData[1].strip()\r\n",
							"\r\n",
							"# Crear dataframe para informacion de cliente\r\n",
							"\r\n",
							"schemaCliente = StructType([ \\\r\n",
							"    StructField(\"Nombre\",StringType(),True), \\\r\n",
							"    StructField(\"Email\",StringType(),True), \\\r\n",
							"    StructField(\"Direccion\",StringType(),True), \\\r\n",
							"    StructField(\"Ciudad_1\",StringType(),True), \\\r\n",
							"    StructField(\"Ciudad_2\", StringType(), True), \\\r\n",
							"    StructField(\"File_name\", StringType(), True) \\\r\n",
							"  ])\r\n",
							"\r\n",
							"data_Cliente = [(Nombre, Email, Direccion, Ciudad_1, Ciudad_2, name)]\r\n",
							"\r\n",
							"df_Cliente = spark.createDataFrame(data=data_Cliente,schema=schemaCliente)\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Obtener Informacion de Prodcuto\r\n",
							"\r\n",
							"TipoProducto = 'Fiducuenta'\r\n",
							"NumeroProducto = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 19).first()[0]\r\n",
							"Desde = df_formated.select(regexp_replace(df_formated.contenido, '\\D', '')).where(df_formated.UniqueId == 12).first()[0]\r\n",
							"Hasta = df_formated.select(regexp_replace(df_formated.contenido, '\\D', '')).where(df_formated.UniqueId == 20).first()[0]\r\n",
							"ValorUnidad = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 21).first()[0]\r\n",
							"\r\n",
							"Rentabilidad = df_formated.select(regexp_replace(df_formated.contenido, '[aA-zZ]|%|\\s', '')).where(df_formated.UniqueId == 22).first()[0]\r\n",
							"if \"-\" in Rentabilidad: Rentabilidad = \"-\"+Rentabilidad.replace(\"-\",\"\")\r\n",
							"\r\n",
							"Comision = df_formated.select(regexp_replace(df_formated.contenido, '[aA-zZ]|%|\\s', '')).where(df_formated.UniqueId == 17).first()[0]\r\n",
							"\r\n",
							"\r\n",
							"# Crear dataframe para informacion de producto\r\n",
							"\r\n",
							"schemaProducto = StructType([ \\\r\n",
							"    StructField(\"TipoProducto\",StringType(),True), \\\r\n",
							"    StructField(\"NumeroProducto\",StringType(),True), \\\r\n",
							"    StructField(\"Desde\",StringType(),True), \\\r\n",
							"    StructField(\"Hasta\",StringType(),True), \\\r\n",
							"    StructField(\"ValorUnidad\",StringType(),True), \\\r\n",
							"    StructField(\"Rentabilidad\",StringType(),True), \\\r\n",
							"    StructField(\"Comision\",StringType(),True), \\\r\n",
							"    StructField(\"File_name\", StringType(), True)\r\n",
							"  ])\r\n",
							"\r\n",
							"data_Producto = [(TipoProducto, NumeroProducto, Desde, Hasta, ValorUnidad, Rentabilidad, Comision, name)]\r\n",
							"\r\n",
							"df_Producto = spark.createDataFrame(data=data_Producto,schema=schemaProducto)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 92
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Extraer informacion de transacciones\r\n",
							"\r\n",
							"# Definir incrementales basado en el uniqueid\r\n",
							"\r\n",
							"Incremental = Window.orderBy(\"UniqueId\")\r\n",
							"\r\n",
							"#Extraer Fecha de transacciones\r\n",
							"\r\n",
							"df_Fecha = df_formated.select(regexp_replace(df_formated.contenido, '[aA-zZ]|%|\\s', '').alias(\"FechaTransaccion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x0 >= 27.0) & (df_formated.x0 <= 28.0)) & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_Fecha = df_Fecha.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"# Obtener cantidad de transacciones\r\n",
							"CantidadTransacciones = df_Fecha.count()\r\n",
							"\r\n",
							"#Extraer Descripcion de transacciones\r\n",
							"\r\n",
							"df_Descripcion = df_formated.select(regexp_replace(df_formated.contenido, '\\d', '').alias(\"Descripcion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x0 >= 27.0) & (df_formated.x0 <= 28.0)) & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_Descripcion = df_Descripcion.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"#Extraer Valor en $\r\n",
							"\r\n",
							"df_ValorPesos = df_formated.select(df_formated.contenido.alias(\"ValorPesos\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x0 >= 343.0) & (df_formated.x0 <= 344.0)) & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_ValorPesos = df_ValorPesos.withColumn(\"ValorPesos\", regexp_replace(df_ValorPesos.ValorPesos, '[.]', ''))  \r\n",
							"df_ValorPesos = df_ValorPesos.withColumn(\"ValorPesos\", regexp_replace(df_ValorPesos.ValorPesos, ',', '.'))\r\n",
							"df_ValorPesos = df_ValorPesos.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"#Extraer Valor en unidades\r\n",
							"\r\n",
							"df_ValorUnidades = df_formated.select(df_formated.contenido.alias(\"ValorUnidades\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x1 >= 501.5) & (df_formated.x1 <= 502.5)) & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_ValorUnidades = df_ValorUnidades.withColumn(\"ValorUnidades\", regexp_replace(df_ValorUnidades.ValorUnidades, '[.]', ''))  \r\n",
							"df_ValorUnidades = df_ValorUnidades.withColumn(\"ValorUnidades\", regexp_replace(df_ValorUnidades.ValorUnidades, ',', '.'))\r\n",
							"df_ValorUnidades = df_ValorUnidades.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"#Extraer Saldo\r\n",
							"\r\n",
							"df_Saldo = df_formated.select(df_formated.contenido.alias(\"Saldo\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x1 >= 593.5) & (df_formated.x1 <= 594.5)) & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_Saldo = df_Saldo.withColumn(\"Saldo\", regexp_replace(df_Saldo.Saldo, '[.]', ''))  \r\n",
							"df_Saldo = df_Saldo.withColumn(\"Saldo\", regexp_replace(df_Saldo.Saldo, ',', '.'))\r\n",
							"df_Saldo = df_Saldo.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"#Agregar pdf name \r\n",
							"\r\n",
							"schema_pdf_name = StructType([\r\n",
							"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"pdfname\", StringType(), nullable=True)    \r\n",
							"])\r\n",
							"\r\n",
							"df_file_name = spark.createDataFrame([], schema_pdf_name)\r\n",
							"\r\n",
							"for i in range (CantidadTransacciones):                \r\n",
							"    newRow = spark.createDataFrame([(i+1,name)])\r\n",
							"    df_file_name = df_file_name.union(newRow) \r\n",
							""
						],
						"outputs": [],
						"execution_count": 93
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_Fecha.show()\r\n",
							"#df_Descripcion.show()\r\n",
							"#df_ValorPesos.show()\r\n",
							"#df_ValorUnidades.show()\r\n",
							"#df_Saldo.show()"
						],
						"outputs": [],
						"execution_count": 94
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_Transacciones = df_Fecha.join(df_Descripcion, df_Fecha.id == df_Descripcion.id, \"inner\") \\\r\n",
							"                  .join(df_ValorPesos , df_Fecha.id == df_ValorPesos.id, \"inner\") \\\r\n",
							"                  .join(df_ValorUnidades, df_Fecha.id == df_ValorUnidades.id, \"inner\") \\\r\n",
							"                  .join(df_Saldo, df_Fecha.id == df_Saldo.id, \"inner\") \\\r\n",
							"                  .join(df_file_name, df_Fecha.id == df_file_name.id, \"inner\") \\\r\n",
							"                  .select( \\\r\n",
							"                        df_Fecha.FechaTransaccion, \\\r\n",
							"                        df_Descripcion.Descripcion, \\\r\n",
							"                        df_ValorPesos.ValorPesos, \\\r\n",
							"                        df_ValorUnidades.ValorUnidades, \\\r\n",
							"                        df_Saldo.Saldo, \\\r\n",
							"                        df_file_name.pdfname\r\n",
							"                        )"
						],
						"outputs": [],
						"execution_count": 95
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#display(df_Cliente)\r\n",
							"#display(df_Producto)\r\n",
							"display(df_Transacciones)"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Generar archivos con informacion\r\n",
							"\r\n",
							"Routeadl = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net\"\r\n",
							"path_sink_comp = fecha+\"/\"+user+\"/\"+name\r\n",
							"\r\n",
							"# crear parquet file para cliente\r\n",
							"df_Cliente.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Cliente/\"+path_sink_comp)\r\n",
							"\r\n",
							"# crear parquet file para porducto\r\n",
							"df_Producto.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Producto/\"+path_sink_comp)\r\n",
							"\r\n",
							"# crear parquet file para transacciones\r\n",
							"df_Transacciones.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Transacciones/\"+path_sink_comp)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 97
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Procesar Data Cruda PDF - Zona Data - Tarjeta Credito')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "LecturaScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "9df28662-2069-46a5-b643-7e14919748cd"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"#Importar Librerias Necesarias\r\n",
							"\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\r\n",
							"from pyspark.sql.window import Window\r\n",
							"from pyspark.sql.functions import *"
						],
						"outputs": [],
						"execution_count": 79
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"#Configurar parametros aqui\r\n",
							"\r\n",
							"user = \"bsmoralesg@outlook.com\"\r\n",
							"type = \"Tarjeta Credito\"\r\n",
							"fecha = \"10012023\"\r\n",
							"\r\n",
							"#name = \"Extracto_403340132_202305_TARJETA_VISA_2166\"\r\n",
							"#name = \"Extracto_7358651_202108_TARJETA_VISA_2166\"\r\n",
							"#name = \"Extracto_392576648_202304_TARJETA_VISA_2166\"\r\n",
							"#name = \"Extracto_369280261_202303_TARJETA_VISA_2166\"\r\n",
							"#name = \"Extracto_231339919_202209_TARJETA_VISA_2166\"\r\n",
							"name = \"Extracto_204864725_202207_TARJETA_VISA_2166\""
						],
						"outputs": [],
						"execution_count": 80
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Configurar rutas de origen de archivos\r\n",
							"\r\n",
							"pathsource = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net/\"+type+\"/Raw/\"+fecha+\"/\"+user+\"/\"+name\r\n",
							""
						],
						"outputs": [],
						"execution_count": 81
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#Obtener Datos para Tarjeta Credito\r\n",
							"\r\n",
							"DataCruda = spark.read.load(pathsource+\"/*.parquet\", format='parquet')"
						],
						"outputs": [],
						"execution_count": 82
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Dividir los valores por salto de linea\r\n",
							"\r\n",
							"df_split = DataCruda.withColumn(\"contenido\", split(DataCruda[\"contenido\"], \"\\n\"))\r\n",
							"\r\n",
							"# Transformar la columna array para crear una nueva fila por cada elemento\r\n",
							"df_formated = df_split.select(df_split.index, df_split.pagenumber, df_split.x0, df_split.x1, df_split.y0, df_split.y1, explode(\"contenido\").alias(\"contenido\"))\r\n",
							"\r\n",
							"#Crear identificador unico  basado enel orden de lectura\r\n",
							"\r\n",
							"Incremental = Window.orderBy(\"pagenumber\",\"index\")\r\n",
							"df_formated = df_formated.withColumn(\"UniqueId\", row_number().over(Incremental))"
						],
						"outputs": [],
						"execution_count": 83
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"display (df_formated)"
						],
						"outputs": [],
						"execution_count": 84
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Obtener Informacion de Cliente\r\n",
							"\r\n",
							"Nombre = df_formated.select(df_formated.contenido).where((df_formated.UniqueId == 2)).first()[0]\r\n",
							"Email = user\r\n",
							"Direccion = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 4).first()[0]\r\n",
							"Ciudad_1 = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 5).first()[0]\r\n",
							"Ciudad_2 = df_formated.select(df_formated.contenido).where(df_formated.UniqueId == 6).first()[0]\r\n",
							"\r\n",
							"# Crear dataframe para informacion de cliente\r\n",
							"\r\n",
							"schemaCliente = StructType([ \\\r\n",
							"    StructField(\"Nombre\",StringType(),True), \\\r\n",
							"    StructField(\"Email\",StringType(),True), \\\r\n",
							"    StructField(\"Direccion\",StringType(),True), \\\r\n",
							"    StructField(\"Ciudad_1\",StringType(),True), \\\r\n",
							"    StructField(\"Ciudad_2\", StringType(), True), \\\r\n",
							"    StructField(\"File_name\", StringType(), True) \\\r\n",
							"  ])\r\n",
							"\r\n",
							"data_Cliente = [(Nombre, Email, Direccion, Ciudad_1, Ciudad_2, name)]\r\n",
							"\r\n",
							"df_Cliente = spark.createDataFrame(data=data_Cliente,schema=schemaCliente)"
						],
						"outputs": [],
						"execution_count": 85
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Obtener Informacion de Prodcuto\r\n",
							"\r\n",
							"\r\n",
							"TipoProducto = 'Tarjeta Credito'\r\n",
							"CupoTotal =  df_formated.select(df_formated.contenido).where(df_formated.x0 == 65.0).first()[0]\r\n",
							"NumeroTarjeta = df_formated.select(df_formated.contenido).where(df_formated.x0 == 465.0).first()[0]\r\n",
							"TasaInteresMVCompra1Mes = df_formated.select(df_formated.contenido).where(df_formated.x0 == 113.0).collect()[0].__getitem__('contenido')\r\n",
							"TasaInteresMVCompra2MesOMas = df_formated.select(df_formated.contenido).where(df_formated.x0 == 113.0).collect()[1].__getitem__('contenido')\r\n",
							"TasaInteresMVImpuestos = df_formated.select(df_formated.contenido).where(df_formated.x0 == 113.0).collect()[2].__getitem__('contenido')\r\n",
							"TasaInteresMVAvances = df_formated.select(df_formated.contenido).where(df_formated.x0 == 113.0).collect()[3].__getitem__('contenido')\r\n",
							"TasaInteresMVMora = df_formated.select(df_formated.contenido).where(df_formated.x0 == 113.0).collect()[4].__getitem__('contenido')\r\n",
							"TasaInteresEACompra1Mes = df_formated.select(df_formated.contenido).where(df_formated.x0 == 153.0).collect()[0].__getitem__('contenido')\r\n",
							"TasaInteresEACompra2MesOMas = df_formated.select(df_formated.contenido).where(df_formated.x0 == 153.0).collect()[1].__getitem__('contenido')\r\n",
							"TasaInteresEAImpuestos = df_formated.select(df_formated.contenido).where(df_formated.x0 == 153.0).collect()[2].__getitem__('contenido')\r\n",
							"TasaInteresEAAvances = df_formated.select(df_formated.contenido).where(df_formated.x0 == 153.0).collect()[3].__getitem__('contenido')\r\n",
							"TasaInteresEAMora = df_formated.select(df_formated.contenido).where(df_formated.x0 == 153.0).collect()[4].__getitem__('contenido')\r\n",
							"\r\n",
							"# Crear dataframe para informacion de producto\r\n",
							"\r\n",
							"schemaProducto = StructType([ \\\r\n",
							"    StructField(\"TipoProducto\",StringType(),True), \\\r\n",
							"    StructField(\"CupoTotal\",StringType(),True), \\\r\n",
							"    StructField(\"NumeroTarjeta\",StringType(),True), \\\r\n",
							"    StructField(\"TasaInteresMVCompra1Mes\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresMVCompra2MesOMas\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresMVImpuestos\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresMVAvances\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresMVMora\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresEACompra1Mes\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresEACompra2MesOMas\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresEAImpuestos\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresEAAvances\", StringType(), True), \\\r\n",
							"    StructField(\"TasaInteresEAMora\", StringType(), True), \\\r\n",
							"    StructField(\"File_name\", StringType(), True) \\\r\n",
							"  ])\r\n",
							"\r\n",
							"data_Producto = [(TipoProducto, CupoTotal, NumeroTarjeta, TasaInteresMVCompra1Mes, TasaInteresMVCompra2MesOMas, TasaInteresMVImpuestos, \\\r\n",
							"    TasaInteresMVAvances, TasaInteresMVMora, TasaInteresEACompra1Mes, TasaInteresEACompra1Mes, TasaInteresEAImpuestos, TasaInteresEAAvances, TasaInteresEAMora, name\r\n",
							")]\r\n",
							"\r\n",
							"df_Producto = spark.createDataFrame(data=data_Producto,schema=schemaProducto)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 86
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"# Extraer informaicon de transacciones\r\n",
							"\r\n",
							"# Definir incrementales basado en el uniqueid\r\n",
							"\r\n",
							"Incremental = Window.orderBy(\"UniqueId\")\r\n",
							"\r\n",
							"#Extraer id de transacciones\r\n",
							"\r\n",
							"df_idTransacciones = df_formated.select(df_formated.contenido.alias(\"IdTransaccion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x0 >= 33.0) & (df_formated.x0 <= 35.0)) & \\\r\n",
							"        (df_formated.contenido != \"Número de\")  & \\\r\n",
							"        (df_formated.contenido != \"Autorización\")  & \\\r\n",
							"        (df_formated.contenido != \"\") \r\n",
							"    )\r\n",
							"df_idTransacciones = df_idTransacciones.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"# Obtener cantidad de transacciones\r\n",
							"CantidadTransacciones = df_idTransacciones.count()\r\n",
							"\r\n",
							"# Extraer fecha de transacciones\r\n",
							"\r\n",
							"df_FechaTransacciones = df_formated.select(df_formated.contenido.alias(\"FechaTransaccion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x0 >= 90.0) & (df_formated.x0 <= 92.0)) & \\\r\n",
							"        (df_formated.contenido != \"Transacción\")  & \\\r\n",
							"        (df_formated.contenido != \"Fecha de\")  & \\\r\n",
							"        (df_formated.contenido != \"ESTADO DE CUENTA EN:\")  & \\\r\n",
							"        (df_formated.contenido != \"\") \r\n",
							"    )\r\n",
							"df_FechaTransacciones = df_FechaTransacciones.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer descripcion de transacciones\r\n",
							"\r\n",
							"df_DescripcionTransacciones = df_formated.select(df_formated.contenido.alias(\"DescripcionTransaccion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        (df_formated.x0 == 135.0) & \\\r\n",
							"        (df_formated.contenido != \"ESTADO DE CUENTA EN:\") & \\\r\n",
							"        (df_formated.contenido != \"Pactada\") & \\\r\n",
							"        (~ df_formated.contenido.contains('VR MONEDA ORIG')) & \\\r\n",
							"        (df_formated.contenido != \"\") \r\n",
							"    )\r\n",
							"df_DescripcionTransacciones = df_DescripcionTransacciones.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer Costo de transacciones\r\n",
							"\r\n",
							"df_CostoTransacciones = df_formated.select(df_formated.contenido.alias(\"CostoTransaccion\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"        ((df_formated.x0 >= 307.0) & (df_formated.x0 <= 317)) & \\\r\n",
							"        (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_CostoTransacciones = df_CostoTransacciones.withColumn('CostoTransaccion', \\\r\n",
							"        when(df_CostoTransacciones.CostoTransaccion.contains('-'), concat(lit(\"-\"), regexp_replace(df_CostoTransacciones.CostoTransaccion, '-', ''))) \\\r\n",
							"        .otherwise(df_CostoTransacciones.CostoTransaccion)\r\n",
							"    )\r\n",
							"df_CostoTransacciones = df_CostoTransacciones.withColumn(\"CostoTransaccion\", regexp_replace(df_CostoTransacciones.CostoTransaccion, ',', ''))\r\n",
							"df_CostoTransacciones = df_CostoTransacciones.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer Tasa Pactada\r\n",
							"\r\n",
							"df_TasaPactada = df_formated.select(df_formated.contenido.alias(\"TasaPactada\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"            (df_formated.x0 == 354.00) & \\\r\n",
							"            (df_formated.contenido != \"Pactada\") & \\\r\n",
							"            (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"\r\n",
							"# Completar datos Faltantes\r\n",
							"\r\n",
							"if (df_TasaPactada.count() !=  CantidadTransacciones):\r\n",
							"    MaximaCantidadRegistros = df_TasaPactada.count()\r\n",
							"    RegistrosPendintes = CantidadTransacciones - MaximaCantidadRegistros\r\n",
							"    MaxUniqueID = df_TasaPactada.select(max(df_TasaPactada.UniqueId)).first()[0]\r\n",
							"    \r\n",
							"    if MaxUniqueID is None:\r\n",
							"        MaxUniqueID = 1\r\n",
							"\r\n",
							"    for i in range (RegistrosPendintes):                \r\n",
							"        newRow = spark.createDataFrame([(\"\",str(MaxUniqueID+(i+1)))])\r\n",
							"        df_TasaPactada = df_TasaPactada.union(newRow)\r\n",
							"\r\n",
							"df_TasaPactada = df_TasaPactada.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer Tasa Facturada\r\n",
							"\r\n",
							"df_TasaFacturada = df_formated.select(df_formated.contenido.alias(\"TasaFacturada\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"            (df_formated.x0 == 389.00) & \\\r\n",
							"            (df_formated.contenido != \"Tasa EA\") & \\\r\n",
							"            (df_formated.contenido != \"Facturada\") & \\\r\n",
							"            (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"\r\n",
							"# Completar datos Faltantes\r\n",
							"\r\n",
							"if (df_TasaFacturada.count() !=  CantidadTransacciones):\r\n",
							"    MaximaCantidadRegistros = df_TasaFacturada.count()\r\n",
							"    RegistrosPendintes = CantidadTransacciones - MaximaCantidadRegistros\r\n",
							"    MaxUniqueID = df_TasaFacturada.select(max(df_TasaFacturada.UniqueId)).first()[0]\r\n",
							"    \r\n",
							"    if MaxUniqueID is None:\r\n",
							"        MaxUniqueID = 1\r\n",
							"    \r\n",
							"    for i in range (RegistrosPendintes):                \r\n",
							"        newRow = spark.createDataFrame([(\"\",str(MaxUniqueID+(i+1)))])\r\n",
							"        df_TasaFacturada = df_TasaFacturada.union(newRow)\r\n",
							"\r\n",
							"df_TasaFacturada = df_TasaFacturada.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer Cargos Y Abonos\r\n",
							"\r\n",
							"df_CargosYAbonos = df_formated.select(df_formated.contenido.alias(\"CargosYAbonos\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"\r\n",
							"            ((df_formated.x0 >= 448.90) & (df_formated.x0 <= 460.0)) & \\\r\n",
							"            (df_formated.contenido.rlike(\"[0-9]\")) & \\\r\n",
							"            (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_CargosYAbonos = df_CargosYAbonos.withColumn('CargosYAbonos', \\\r\n",
							"        when(df_CargosYAbonos.CargosYAbonos.contains('-'), concat(lit(\"-\"), regexp_replace(df_CargosYAbonos.CargosYAbonos, '-', ''))) \\\r\n",
							"        .otherwise(df_CargosYAbonos.CargosYAbonos)\r\n",
							"    )\r\n",
							"df_CargosYAbonos = df_CargosYAbonos.withColumn(\"CargosYAbonos\", regexp_replace(df_CargosYAbonos.CargosYAbonos, ',', ''))\r\n",
							"df_CargosYAbonos = df_CargosYAbonos.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer Saldo A diferir\r\n",
							"\r\n",
							"df_SaldoADiferir = df_formated.select(df_formated.contenido.alias(\"SaldoADiferir\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"            (df_formated.x0 == 540.33) &             \r\n",
							"            (df_formated.contenido != \"\")\r\n",
							"    )\r\n",
							"df_SaldoADiferir = df_SaldoADiferir.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"# Extraer Cuotas\r\n",
							"\r\n",
							"df_Cuotas = df_formated.select(df_formated.contenido.alias(\"Cuotas\"), df_formated.UniqueId).where \\\r\n",
							"    ( \\\r\n",
							"            (df_formated.x0 >= 567.0) & (df_formated.x0 <= 571.0) &\r\n",
							"            (df_formated.contenido != \"\")                    \r\n",
							"    )\r\n",
							"df_Cuotas = df_Cuotas.withColumn(\"id\", row_number().over(Incremental))\r\n",
							"\r\n",
							"\r\n",
							"#Trear las descripciones para comparar con ellas eliminando los blancos que se generan en las cuotas\r\n",
							"\r\n",
							"df_DescripcionsFiltradas = df_DescripcionTransacciones.select(df_DescripcionTransacciones.DescripcionTransaccion, \\\r\n",
							"    df_DescripcionTransacciones.UniqueId, df_DescripcionTransacciones.id.alias(\"OriginalID\") ).where \\\r\n",
							"    ( \\\r\n",
							"            (~ df_DescripcionTransacciones.DescripcionTransaccion.contains('ABONO')) &\r\n",
							"            (~ df_DescripcionTransacciones.DescripcionTransaccion.contains('CUOTA DE MANEJO'))                    \r\n",
							"    )\r\n",
							"df_DescripcionsFiltradas = df_DescripcionsFiltradas.withColumn(\"id\", row_number().over(Incremental))  \r\n",
							"df_DescripcionsFiltradas = df_DescripcionsFiltradas.drop(\"UniqueId\")\r\n",
							"\r\n",
							"# Validar contra descripciones filtradas y actualizar - obtener real id de cuota\r\n",
							"\r\n",
							"df_Cuotas = df_DescripcionsFiltradas.join(df_Cuotas, df_DescripcionsFiltradas.id == df_Cuotas.id, \"inner\") \\\r\n",
							"    .select(df_Cuotas.Cuotas, df_Cuotas.UniqueId, df_DescripcionsFiltradas.OriginalID.alias(\"id\"))\r\n",
							"\r\n",
							"# Usar Left Join para consolidar Cuotas (con blancos y sin blancos)\r\n",
							"\r\n",
							"df_DescripcionTransacciones_aux = df_DescripcionTransacciones.select(df_DescripcionTransacciones.id) \r\n",
							"df_Cuotas = df_DescripcionTransacciones_aux.join(df_Cuotas, df_DescripcionTransacciones.id == df_Cuotas.id, \"left\") \\\r\n",
							"    .select(df_Cuotas.Cuotas, df_Cuotas.UniqueId, df_DescripcionTransacciones.id) \\\r\n",
							"    .fillna(0).fillna(\"\") \r\n",
							"\r\n",
							"\r\n",
							"#Agregar pdf name \r\n",
							"\r\n",
							"schema_pdf_name = StructType([\r\n",
							"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"pdfname\", StringType(), nullable=True)    \r\n",
							"])\r\n",
							"\r\n",
							"df_file_name = spark.createDataFrame([], schema_pdf_name)\r\n",
							"\r\n",
							"for i in range (CantidadTransacciones):                \r\n",
							"    newRow = spark.createDataFrame([(i+1,name)])\r\n",
							"    df_file_name = df_file_name.union(newRow) \r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 87
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#df_idTransacciones.show(50)\r\n",
							"#df_FechaTransacciones.show(50)\r\n",
							"#df_DescripcionTransacciones.show(50)\r\n",
							"#df_CostoTransacciones.show(50)\r\n",
							"#df_TasaPactada.show(50)\r\n",
							"#df_TasaFacturada.show(50)\r\n",
							"#df_CargosYAbonos.show(50)\r\n",
							"#df_SaldoADiferir.show(50)\r\n",
							"#df_Cuotas.show(50)"
						],
						"outputs": [],
						"execution_count": 88
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Consolidar Transacciones\r\n",
							"\r\n",
							"df_Transacciones = df_idTransacciones.join(df_FechaTransacciones, df_idTransacciones.id == df_FechaTransacciones.id, \"inner\") \\\r\n",
							"                  .join(df_DescripcionTransacciones, df_idTransacciones.id == df_DescripcionTransacciones.id, \"inner\") \\\r\n",
							"                  .join(df_CostoTransacciones, df_idTransacciones.id == df_CostoTransacciones.id, \"inner\") \\\r\n",
							"                  .join(df_TasaPactada, df_idTransacciones.id == df_TasaPactada.id, \"inner\") \\\r\n",
							"                  .join(df_TasaFacturada, df_idTransacciones.id == df_TasaFacturada.id, \"inner\") \\\r\n",
							"                  .join(df_CargosYAbonos, df_idTransacciones.id == df_CargosYAbonos.id, \"inner\") \\\r\n",
							"                  .join(df_SaldoADiferir, df_idTransacciones.id == df_SaldoADiferir.id, \"inner\") \\\r\n",
							"                  .join(df_Cuotas, df_idTransacciones.id == df_Cuotas.id, \"inner\") \\\r\n",
							"                  .join(df_file_name, df_idTransacciones.id == df_file_name.id, \"inner\") \\\r\n",
							"                  .select( \\\r\n",
							"                        df_idTransacciones.IdTransaccion, \\\r\n",
							"                        df_FechaTransacciones.FechaTransaccion, \\\r\n",
							"                        df_DescripcionTransacciones.DescripcionTransaccion, \\\r\n",
							"                        df_CostoTransacciones.CostoTransaccion, \\\r\n",
							"                        df_TasaPactada.TasaPactada, \\\r\n",
							"                        df_TasaFacturada.TasaFacturada, \\\r\n",
							"                        df_CargosYAbonos.CargosYAbonos, \\\r\n",
							"                        df_SaldoADiferir.SaldoADiferir, \\\r\n",
							"                        df_Cuotas.Cuotas, \\\r\n",
							"                        df_file_name.pdfname\r\n",
							"                        )"
						],
						"outputs": [],
						"execution_count": 89
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#display(df_Cliente)\r\n",
							"#display(df_Producto)\r\n",
							"display(df_Transacciones)"
						],
						"outputs": [],
						"execution_count": 90
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Generar archivos con informacion\r\n",
							"\r\n",
							"Routeadl = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net\"\r\n",
							"path_sink_comp = fecha+\"/\"+user+\"/\"+name\r\n",
							"\r\n",
							"# crear parquet file para cliente\r\n",
							"df_Cliente.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Cliente/\"+path_sink_comp)\r\n",
							"\r\n",
							"# crear parquet file para porducto\r\n",
							"df_Producto.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Producto/\"+path_sink_comp)\r\n",
							"\r\n",
							"# crear parquet file para transacciones\r\n",
							"df_Transacciones.write.mode(\"overwrite\").parquet(Routeadl+\"/\"+type+\"/Transacciones/\"+path_sink_comp)\r\n",
							""
						],
						"outputs": [],
						"execution_count": 91
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Reading PDF File - Test Concept')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SandBoxScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c90baa01-a231-47f8-9d52-8eb2a1e7c0f0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"#Procesamiento con PyPDF2\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from io import BytesIO\r\n",
							"import PyPDF2\r\n",
							"\r\n",
							"pdf_path = \"abfss://stagefiles@datalakeproyectomaestria.dfs.core.windows.net/Extracto_7358651_202108_TARJETA_VISA_2166.pdf\"\r\n",
							"pdf_password = \"1233491047\"\r\n",
							"\r\n",
							"# Read the PDF file as bytes\r\n",
							"pdf_bytes = spark.read \\\r\n",
							"    .format(\"binaryFile\") \\\r\n",
							"    .option(\"path\", pdf_path) \\\r\n",
							"    .load() \\\r\n",
							"    .select(\"content\") \\\r\n",
							"    .collect()[0][0]\r\n",
							"\r\n",
							"# Create a BytesIO stream from the PDF bytes\r\n",
							"pdf_stream = BytesIO(pdf_bytes)\r\n",
							"pdf_stream.seek(0)  # Ensure the stream is at the beginning\r\n",
							"\r\n",
							"# Create a PdfReader object from the PDF stream\r\n",
							"pdf_reader = PyPDF2.PdfReader(pdf_stream)\r\n",
							"\r\n",
							"# Check if the PDF file is encrypted (password-protected)\r\n",
							"if pdf_reader.is_encrypted:\r\n",
							"    # Decrypt the PDF file with the provided password\r\n",
							"    pdf_reader.decrypt(pdf_password)\r\n",
							"\r\n",
							"# Extract text from the PDF pages\r\n",
							"pdf_content = \"\"\r\n",
							"for page_num in range(len(pdf_reader.pages)):\r\n",
							"    page = pdf_reader.pages[page_num]\r\n",
							"    pdf_content += page.extract_text()\r\n",
							"\r\n",
							"print(pdf_content)\r\n",
							"\r\n",
							"# Perform further processing or analysis on the content\r\n",
							"# For example, you can save it to a storage location or perform text mining operations.\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#PROCESAMIENTO CON Rapid Miner\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from io import BytesIO\r\n",
							"from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\r\n",
							"from pdfminer.pdfpage import PDFPage\r\n",
							"from pdfminer.converter import TextConverter\r\n",
							"from pdfminer.layout import LAParams\r\n",
							"import io\r\n",
							"\r\n",
							"spark = SparkSession.builder \\\r\n",
							"    .appName(\"PDF Extraction\") \\\r\n",
							"    .getOrCreate()\r\n",
							"\r\n",
							"pdf_path = \"abfss://stagefiles@datalakeproyectomaestria.dfs.core.windows.net/Extracto_7358651_202108_TARJETA_VISA_2166.pdf\"\r\n",
							"pdf_password = \"1233491047\"\r\n",
							"\r\n",
							"# Read the PDF file as bytes\r\n",
							"pdf_bytes = spark.read \\\r\n",
							"    .format(\"binaryFile\") \\\r\n",
							"    .option(\"path\", pdf_path) \\\r\n",
							"    .load() \\\r\n",
							"    .select(\"content\") \\\r\n",
							"    .collect()[0][0]\r\n",
							"\r\n",
							"# Create a BytesIO stream from the PDF bytes\r\n",
							"pdf_stream = BytesIO(pdf_bytes)\r\n",
							"pdf_stream.seek(0)  # Ensure the stream is at the beginning\r\n",
							"\r\n",
							"# Create PDFResourceManager object and set parameters\r\n",
							"rsrcmgr = PDFResourceManager()\r\n",
							"retstr = io.StringIO()\r\n",
							"\r\n",
							"# Create TextConverter object for the PDF page interpretation\r\n",
							"laparams = LAParams()\r\n",
							"device = TextConverter(rsrcmgr, retstr, laparams=laparams)\r\n",
							"\r\n",
							"# Create PDFPageInterpreter object\r\n",
							"interpreter = PDFPageInterpreter(rsrcmgr, device)\r\n",
							"\r\n",
							"# Process each page of the PDF file\r\n",
							"password = pdf_password.encode()\r\n",
							"pages = PDFPage.get_pages(pdf_stream, password=password, check_extractable=True)\r\n",
							"for page in pages:\r\n",
							"    interpreter.process_page(page)\r\n",
							"\r\n",
							"# Get the extracted text from the StringIO object\r\n",
							"pdf_content = retstr.getvalue()\r\n",
							"\r\n",
							"# Close the StringIO objects\r\n",
							"device.close()\r\n",
							"retstr.close()\r\n",
							"\r\n",
							"print(pdf_content)\r\n",
							"\r\n",
							"\r\n",
							"# Perform further processing or analysis on the content\r\n",
							"# For example, you can save it to a storage location or perform text mining operations.\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/writte a file')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SandBoxScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "aabd014b-6f70-4d0c-a1bd-7dea8ca21cc0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
							"data2 = [(\"James\",\"Smith\",\"Joe\",\"4355\",\"M\",3000),\r\n",
							"    (\"Michael\",\"Rose\",\"Edward\",\"40288\",\"F\",4000)\r\n",
							"  ]\r\n",
							"\r\n",
							"schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"middlename\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", StringType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"    StructField(\"salary\", IntegerType(), True) \\\r\n",
							"  ])\r\n",
							" \r\n",
							"df = spark.createDataFrame(data=data2,schema=schema)\r\n",
							"\r\n",
							"df.write.csv(\"abfss://stagefiles@datalakeproyectomaestria.dfs.core.windows.net/validate_permissions.csv\")"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkConfiguration1')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.jars.packages": "org.apache.tika:tika-parsers:1.26"
				},
				"created": "2023-05-30T23:19:10.1900000-05:00",
				"createdBy": "brallamsantiago.morales334@comunidadunir.net",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.jars.packages": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spproymaestria')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 19,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"libraryRequirements": {
					"content": "name: proyectomaestiraenv\r\nchannels:\r\n- defaults\r\ndependencies:\r\n- pip:\r\n  - Tika\r\n  - PyPDF2\r\n  - pdfminer",
					"filename": "enviroment.yml",
					"time": "2023-06-06T05:12:59.1874798Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpoolproyectomaestria')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/TBL_modelo_transacciones_Cliente')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\nCreate table modelo_transacciones.Cliente (\n    Cliente_Key bigint identity (100,1) not null,\n    Nombre varchar (100) not null,\n    [User] varchar(150) not null,\n    Direccion varchar(200) not null,\n    Ciudad varchar(150)  not null,\n    Departamento varchar(150) not null,\n    Fecha_Insert DATETIME NOT NULL,\n    ValidoDesde DATETIME not null,\n    ValidoHasta DATETIME null\n)\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SP_Manage_Dim_Cliente')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n\n-- Procesamiento informacion clientes\n\n    --Extraer Fechas de PDF - Todas las entidades de cliente\n\n    IF OBJECT_ID(N'tempdb..#Data_Fechas') IS NOT NULL\n    BEGIN\n        DROP TABLE #Customer\n    END\n\n    Select \n        File_name, \n        Fecha = convert(date,Fecha+'01')\n    into #Data_Fechas    \n    FROM \n        (\n            SELECT \n                mt.File_name,\n                ss.value AS Fecha\n            FROM \n                stage.CuentaAhorros_Cliente mt\n            CROSS APPLY STRING_SPLIT(mt.File_name, '_') ss\n            UNION ALL\n            SELECT \n                mt.File_name,\n                ss.value AS Fecha\n            FROM \n                stage.FondoInversion_Cliente mt\n            CROSS APPLY STRING_SPLIT(mt.File_name, '_') ss\n            UNION ALL\n            SELECT \n                mt.File_name,\n                ss.value AS Fecha\n            FROM \n                stage.TarjetaCredito_Cliente mt\n            CROSS APPLY STRING_SPLIT(mt.File_name, '_') ss\n        ) fechas\n    WHERE LEN(Fecha) = 6\n\n\n    --Consultar informacion de clientes\n\n    IF OBJECT_ID(N'tempdb..#Raw_Clientes') IS NOT NULL\n    BEGIN\n        DROP TABLE #Raw_Clientes\n    END\n\n    select         \n        [Nombre]\n        ,[Email]\n        ,[Direccion]\n        ,[Ciudad_1]\n        ,[Ciudad_2]\n        ,[File_name]          \n        ,Fecha \n        ,ROW_NUMBER ()  OVER ( PARTITION BY Email order by Fecha desc ) Incremental\n    into #Raw_Clientes\n    from\n    (\n        Select \n            cli.[Nombre]\n            ,cli.[Email]\n            ,cli.[Direccion]\n            ,cli.[Ciudad_1]\n            ,cli.[Ciudad_2]\n            ,cli.[File_name]  \n            ,fec.Fecha\n            --,ROW_NUMBER ()  OVER ( PARTITION BY Email order by fec.Fecha desc ) Incremental\n        --into \n        --    #Raw_Clientes\n        from \n        (\n            select \n                [Nombre]\n                ,[Email]\n                ,[Direccion]\n                ,[Ciudad_1]\n                ,[Ciudad_2]\n                ,[File_name]           \n            from stage.CuentaAhorros_Cliente\n            UNION ALL\n            select \n                [Nombre]\n                ,[Email]\n                ,[Direccion]\n                ,[Ciudad_1]\n                ,[Ciudad_2]\n                ,[File_name] \n            from stage.FondoInversion_Cliente\n            UNION ALL\n            select \n                [Nombre]\n                ,[Email]\n                ,[Direccion]\n                ,[Ciudad_1]\n                ,[Ciudad_2]\n                ,[File_name]\n            from stage.TarjetaCredito_Cliente\n        ) cli\n        INNER JOIN #Data_Fechas fec on (fec.File_name = cli.File_name)\n        --UNION ALL\n        --    select 'andres', 'adreacorreo@hotmail.com', 'clle flasa 123', 'boyaca', 'boyaca', 'file123.pdf', '2023-05-06' \n        --    UNION ALL\n        --    select 'pablo', 'pablocorreo@hotmail.com', 'clle flasa 123', 'boyaca', 'boyaca', 'file123.pdf', '2023-05-06' \n        --    UNION ALL\n        --    select 'andres', 'adreacorreo@hotmail.com', 'clle flasa 123', 'boyaca', 'boyaca', 'file123.pdf', '2023-05-06' \n        --    UNION ALL\n        --    select 'andres uppda', 'adreacorreo@hotmail.com', 'clle flasa 123', 'boyaca', 'boyaca', 'file123.pdf', '2023-05-07' \n    ) dat2\n\n\n    -- Insertar data si No existe\n\n    insert into modelo_transacciones.Cliente (Nombre, [User], Direccion, Ciudad, Departamento, Fecha_Insert, ValidoDesde, ValidoHasta)\n    SELECT   \n        [Nombre]\n        ,[Email]\n        ,[Direccion]\n        ,[Ciudad_1]\n        ,[Ciudad_2]        \n        ,DATEADD(HOUR, -5, GETUTCDATE())\n        ,DATEADD(HOUR, -5, GETUTCDATE())\n        ,null\n    FROM \n        #Raw_Clientes raw\n    where \n        raw.Incremental = 1 AND\n        not exists (select top 1 1 from modelo_transacciones.Cliente cli where cli.[User] = raw.[Email])\n\n    --Actualizar si existe las fechas de los registors vigentes anteriores\n\n    UPDATE \n        cli\n    SET \n        cli.ValidoHasta = DATEADD(HOUR, -5, GETUTCDATE())\n    FROM \n        modelo_transacciones.Cliente cli\n    INNER JOIN \n        #Raw_Clientes raw\n        on \n            (cli.[User] = raw.[Email])\n     where \n        exists (select top 1 1 from modelo_transacciones.Cliente cli where cli.[User] = raw.[Email])\n        and raw.Incremental = 1\n        and cli.validoHasta  is null\n        and (cli.[Nombre] <> raw.[Nombre] or cli.[Direccion] <> raw.[Direccion] or cli.[Ciudad] <> raw.Ciudad_1 or cli.[Departamento] <> raw.Ciudad_2)\n\n\n    -- insertar nuevo registro de cambio\n\n    insert into modelo_transacciones.Cliente (Nombre, [User], Direccion, Ciudad, Departamento, Fecha_Insert, ValidoDesde, ValidoHasta)\n    SELECT   \n        raw.[Nombre]\n        ,raw.[Email]\n        ,raw.[Direccion]\n        ,raw.[Ciudad_1]\n        ,raw.[Ciudad_2]        \n        ,DATEADD(HOUR, -5, GETUTCDATE())\n        ,DATEADD(HOUR, -5, GETUTCDATE())\n        ,null\n    FROM\n        modelo_transacciones.Cliente cli\n    INNER JOIN\n        #Raw_Clientes raw\n        on \n            (cli.[User] = raw.[Email])\n    where\n        exists (select top 1 1 from modelo_transacciones.Cliente cli where cli.[User] = raw.[Email])\n        and raw.Incremental = 1\n        and (cli.[Nombre] <> raw.[Nombre] or cli.[Direccion] <> raw.[Direccion] or cli.[Ciudad] <> raw.Ciudad_1 or cli.[Departamento] <> raw.Ciudad_2)\n\n\n\n    --select * from modelo_transacciones.Cliente\n\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SCH_modelo_Transacciones')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "Schemas"
				},
				"content": {
					"query": "create schema modelo_transacciones\npt 2",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		}
	]
}