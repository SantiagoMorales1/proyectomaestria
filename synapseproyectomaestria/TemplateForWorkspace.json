{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapseproyectomaestria"
		},
		"synapseproyectomaestria-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapseproyectomaestria-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapseproyectomaestria.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synapseproyectomaestria-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalakeproyectomaestria.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline - Process PDF Files - Stage Zone')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Reafing PDF Files - Stage Zone",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Procesar Archivos PDF - Zona Stage",
								"type": "NotebookReference"
							},
							"parameters": {
								"pdf_user": {
									"value": "bsmoralesg@outlook.com",
									"type": "string"
								},
								"pdf_type": {
									"value": "Tarjeta Credito",
									"type": "string"
								},
								"pdf_fecha": {
									"value": "10012023",
									"type": "string"
								},
								"file_name": {
									"value": "Extracto_7358651_202108_TARJETA_VISA_2166.pdf",
									"type": "string"
								},
								"pdf_password": {
									"value": "1233491047",
									"type": "string"
								}
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "spproymaestria",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": false,
								"spark.dynamicAllocation.minExecutors": 1,
								"spark.dynamicAllocation.maxExecutors": 1
							},
							"driverSize": "Small",
							"numExecutors": 1
						}
					},
					{
						"name": "Delete Success File",
						"type": "Delete",
						"dependsOn": [
							{
								"activity": "Reafing PDF Files - Stage Zone",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"dataset": {
								"referenceName": "DataLakeMaster",
								"type": "DatasetReference",
								"parameters": {}
							},
							"enableLogging": false,
							"storeSettings": {
								"type": "AzureBlobFSReadSettings",
								"recursive": true,
								"wildcardFileName": "*_SUCCESS",
								"enablePartitionDiscovery": false
							}
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {},
					"cancelAfter": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Procesar Archivos PDF - Zona Stage')]",
				"[concat(variables('workspaceId'), '/bigDataPools/spproymaestria')]",
				"[concat(variables('workspaceId'), '/datasets/DataLakeMaster')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DataLakeMaster')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapseproyectomaestria-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileSystem": "stagedata"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapseproyectomaestria-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapseproyectomaestria-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapseproyectomaestria-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapseproyectomaestria-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapseproyectomaestria-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/primera carga')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "IF NOT EXISTS (SELECT * FROM sys.external_file_formats WHERE name = 'SynapseDelimitedTextFormat') \n\tCREATE EXTERNAL FILE FORMAT [SynapseDelimitedTextFormat] \n\tWITH ( FORMAT_TYPE = DELIMITEDTEXT ,\n\t       FORMAT_OPTIONS (\n\t\t\t FIELD_TERMINATOR = ',',\n\t\t\t USE_TYPE_DEFAULT = FALSE\n\t\t\t))\nGO\n\nIF NOT EXISTS (SELECT * FROM sys.external_data_sources WHERE name = 'stagedata_datalakeproyectomaestria_dfs_core_windows_net') \n\tCREATE EXTERNAL DATA SOURCE [stagedata_datalakeproyectomaestria_dfs_core_windows_net] \n\tWITH (\n\t\tLOCATION = 'abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net', \n\t\tTYPE = HADOOP \n\t)\nGO\n\nCREATE EXTERNAL TABLE Extractos.Data (\n\t[filename] nvarchar(4000),\n\t[rmse] float,\n\t[spectral_centroid] float,\n\t[spectral_bandwidth] float,\n\t[rolloff] float,\n\t[zero_crossing_rate] float,\n\t[mfcc1] float,\n\t[mfcc2] float,\n\t[mfcc3] float,\n\t[mfcc4] float,\n\t[mfcc5] float,\n\t[mfcc6] float,\n\t[mfcc7] float,\n\t[mfcc8] float,\n\t[mfcc9] float,\n\t[mfcc10] float,\n\t[mfcc11] float,\n\t[mfcc12] float,\n\t[mfcc13] float,\n\t[mfcc14] float,\n\t[mfcc15] float,\n\t[mfcc16] float,\n\t[mfcc17] float,\n\t[mfcc18] float,\n\t[mfcc19] float,\n\t[mfcc20] float,\n\t[label] nvarchar(4000)\n\t)\n\tWITH (\n\tLOCATION = 'dataset.csv',\n\tDATA_SOURCE = [stagedata_datalakeproyectomaestria_dfs_core_windows_net],\n\tFILE_FORMAT = [SynapseDelimitedTextFormat]\n\t)\nGO\n\n\nSELECT TOP 100 * FROM Extractos.Data\nGO",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "sqlpoolproyectomaestria",
						"poolName": "sqlpoolproyectomaestria"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Procesar Archivos PDF - Zona Stage')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "LecturaScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "5fc056a0-0d92-4c0c-bed8-8b7a2434d327"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"#Importando Librerias\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from io import BytesIO\r\n",
							"from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\r\n",
							"from pdfminer.pdfpage import PDFPage\r\n",
							"from pdfminer.converter import TextConverter\r\n",
							"from pdfminer.layout import LAParams\r\n",
							"import io\r\n",
							""
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Creando sesion \r\n",
							"\r\n",
							"spark = SparkSession.builder \\\r\n",
							"    .appName(\"PDF Extraction\") \\\r\n",
							"    .getOrCreate()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": [
								"parameters"
							]
						},
						"source": [
							"#Capturar Parametros\r\n",
							"pdf_user = \"\"\r\n",
							"pdf_type = \"\"\r\n",
							"pdf_fecha = \"\"\r\n",
							"file_name = \"\"\r\n",
							"pdf_password = \"\""
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Consolidar Variables\r\n",
							"\r\n",
							"#Ruta Origen\r\n",
							"pdf_path_origin = \"abfss://stagefiles@datalakeproyectomaestria.dfs.core.windows.net/\"+pdf_user+\"/\"+pdf_type+\"/\"+pdf_fecha+\"/\"+file_name\r\n",
							"\r\n",
							"#Ruta Destino\r\n",
							"file_name = file_name.replace(\".pdf\", \"\")\r\n",
							"path_sink = \"abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net/\"+pdf_user+\"/\"+pdf_type+\"/\"+pdf_fecha+\"/\"+file_name"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Procesar informacion - Capturar informacion\r\n",
							"\r\n",
							"\r\n",
							"# Leer the PDF file como bytes\r\n",
							"pdf_bytes = spark.read \\\r\n",
							"    .format(\"binaryFile\") \\\r\n",
							"    .option(\"path\", pdf_path_origin) \\\r\n",
							"    .load() \\\r\n",
							"    .select(\"content\") \\\r\n",
							"    .collect()[0][0]\r\n",
							"\r\n",
							"# crear un BytesIO stream de the PDF bytes\r\n",
							"pdf_stream = BytesIO(pdf_bytes)\r\n",
							"pdf_stream.seek(0)\r\n",
							"\r\n",
							"# Crear objeto PDFResourceManager y configurar parametros\r\n",
							"rsrcmgr = PDFResourceManager()\r\n",
							"retstr = io.StringIO()\r\n",
							"\r\n",
							"# Crear Objeto  TextConverter para las paginas dle PDF\r\n",
							"laparams = LAParams()\r\n",
							"device = TextConverter(rsrcmgr, retstr, laparams=laparams)\r\n",
							"\r\n",
							"# Crear objeto PDFPageInterpreter\r\n",
							"interpreter = PDFPageInterpreter(rsrcmgr, device)\r\n",
							"\r\n",
							"# Porcesar cada pagina del archivo PDF\r\n",
							"password = pdf_password.encode()\r\n",
							"pages = PDFPage.get_pages(pdf_stream, password=password, check_extractable=True)\r\n",
							"for page in pages:\r\n",
							"    interpreter.process_page(page)\r\n",
							"\r\n",
							"# Obtener el texto extraido del objeto StringIO\r\n",
							"pdf_content = retstr.getvalue()\r\n",
							"\r\n",
							"# Cerrar el objeto StringIO\r\n",
							"device.close()\r\n",
							"retstr.close()"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Crear dataframe\r\n",
							"pdf_df = spark.createDataFrame([(pdf_content,)], [\"contenido\"])\r\n",
							"\r\n",
							"# crear parquet file\r\n",
							"pdf_df.write.mode(\"overwrite\").parquet(path_sink)"
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Procesar Data Cruda PDF - Zona Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "LecturaScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "a7062dd4-ac30-4708-ae59-87ea81728c6b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": false
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"#Importar Librerias Necesarias\r\n",
							"\r\n",
							"from pyspark.sql.functions import split\r\n",
							"from pyspark.sql.functions import col\r\n",
							"from pyspark.sql.functions import size\r\n",
							"from pyspark.sql.functions import explode\r\n",
							"from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType\r\n",
							""
						],
						"outputs": [],
						"execution_count": 192
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"#Obtener Datos para Tarjeta Credito\r\n",
							"\r\n",
							"DataCruda = spark.read.load('abfss://stagedata@datalakeproyectomaestria.dfs.core.windows.net/bsmoralesg@outlook.com/Tarjeta Credito/10012023/Extracto_7358651_202108_TARJETA_VISA_2166/*.parquet', format='parquet')"
						],
						"outputs": [],
						"execution_count": 66
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Dividir los valores por salto de linea\r\n",
							"\r\n",
							"df_split = df.withColumn(\"contenido\", split(df[\"contenido\"], \"\\n\"))\r\n",
							"\r\n",
							"# Transformar la columna array para crear una nueva fila por cada elemento\r\n",
							"df_fromated = df_split.select(explode(\"contenido\").alias(\"contenido\"))\r\n",
							""
						],
						"outputs": [],
						"execution_count": 129
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#ObtenerInfomraicon de Cliente\r\n",
							"\r\n",
							"Nombre = df_fromated.collect()[1].asDict()[\"contenido\"]\r\n",
							"Direccion = df_fromated.collect()[3].asDict()[\"contenido\"]\r\n",
							"Ciudad_1 = df_fromated.collect()[4].asDict()[\"contenido\"]\r\n",
							"Ciudad_2 = df_fromated.collect()[5].asDict()[\"contenido\"]\r\n",
							""
						],
						"outputs": [],
						"execution_count": 130
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Obtener Informacion de Prodcuto\r\n",
							"\r\n",
							"TipoProducto = 'Tarjeta Credito'\r\n",
							"CupoTotal =  df_fromated.collect()[54].asDict()[\"contenido\"]\r\n",
							"NumeroTarjeta = df_fromated.collect()[23].asDict()[\"contenido\"]\r\n",
							"TasaInteresMVCompra1Mes = df_fromated.collect()[93].asDict()[\"contenido\"]\r\n",
							"TasaInteresMVCompra2MesOMas = df_fromated.collect()[94].asDict()[\"contenido\"]\r\n",
							"TasaInteresMVImpuestos = df_fromated.collect()[95].asDict()[\"contenido\"]\r\n",
							"TasaInteresMVAvances = df_fromated.collect()[96].asDict()[\"contenido\"]\r\n",
							"TasaInteresMVMora = df_fromated.collect()[97].asDict()[\"contenido\"]\r\n",
							"TasaInteresEACompra1Mes = df_fromated.collect()[99].asDict()[\"contenido\"]\r\n",
							"TasaInteresEACompra2MesOMas =df_fromated.collect()[100].asDict()[\"contenido\"]\r\n",
							"TasaInteresEAImpuestos = df_fromated.collect()[101].asDict()[\"contenido\"]\r\n",
							"TasaInteresEAAvances = df_fromated.collect()[102].asDict()[\"contenido\"]\r\n",
							"TasaInteresEAMora = df_fromated.collect()[103].asDict()[\"contenido\"]\r\n",
							""
						],
						"outputs": [],
						"execution_count": 134
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Definir estructura para Informacion de ID Transaccion\r\n",
							"\r\n",
							"schema1 = StructType([\r\n",
							"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"IdTransaccion\", StringType(), nullable=True)\r\n",
							"])\r\n",
							"\r\n",
							"df_idTransacciones = spark.createDataFrame([], schema1)\r\n",
							"ArrayIdTransacciones = []\r\n",
							"\r\n",
							"\r\n",
							"#Definir estructura para fecha Transaccion\r\n",
							"\r\n",
							"schema2 = StructType([\r\n",
							"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"FechaTransaccion\", StringType(), nullable=True)\r\n",
							"])\r\n",
							"\r\n",
							"df_FechaTransacciones = spark.createDataFrame([], schema2)\r\n",
							"ArrayFechaTransacciones = []\r\n",
							"\r\n",
							"\r\n",
							"#Definir estructura para fecha Descripciones\r\n",
							"\r\n",
							"schema3 = StructType([\r\n",
							"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"DescripcionTransaccion\", StringType(), nullable=True)\r\n",
							"])\r\n",
							"\r\n",
							"df_DescripcionTransacciones = spark.createDataFrame([], schema3)\r\n",
							"ArrayDescripcionTransacciones = []\r\n",
							"\r\n",
							"\r\n",
							"#Definir estructura para Costo transaccion\r\n",
							"\r\n",
							"schema4 = StructType([\r\n",
							"    StructField(\"id\", IntegerType(), nullable=True),\r\n",
							"    StructField(\"CostoTransaccion\", DoubleType(), nullable=True)\r\n",
							"])\r\n",
							"\r\n",
							"df_CostoTransacciones = spark.createDataFrame([], schema4)\r\n",
							"ArrayCostoTransacciones = []\r\n",
							"\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 203
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Obtener informaicon de transacciones\r\n",
							"\r\n",
							"obtenerIDDataTranscciones = False\r\n",
							"obtenerFechaTranscciones = False\r\n",
							"obtenerDescripcionTranscciones = False\r\n",
							"obtenerCostoTranscciones = False\r\n",
							"\r\n",
							"iteradorIDDataTranscciones = 1\r\n",
							"iteradorFechaDataTranscciones = 1\r\n",
							"iteradorDescripcionTranscciones = 1\r\n",
							"iteradorCostoTranscciones = 1\r\n",
							"\r\n",
							"for row in df_fromated.collect():\r\n",
							"    \r\n",
							"    if (obtenerIDDataTranscciones == True):\r\n",
							"        if (row[\"contenido\"] == \"\"):\r\n",
							"             obtenerIDDataTranscciones = False\r\n",
							"        else:\r\n",
							"            ArrayIdTransacciones.append((iteradorIDDataTranscciones, row[\"contenido\"]))\r\n",
							"            iteradorIDDataTranscciones = iteradorIDDataTranscciones + 1\r\n",
							"\r\n",
							"    if (obtenerFechaTranscciones == True):\r\n",
							"        if (row[\"contenido\"] == \"\"):\r\n",
							"            obtenerFechaTranscciones = False\r\n",
							"        else:\r\n",
							"            ArrayFechaTransacciones.append((iteradorFechaDataTranscciones, row[\"contenido\"]))\r\n",
							"            iteradorFechaDataTranscciones = iteradorFechaDataTranscciones + 1\r\n",
							"\r\n",
							"    if (obtenerDescripcionTranscciones == True):\r\n",
							"        if (row[\"contenido\"] == \"\" and iteradorDescripcionTranscciones != 1):\r\n",
							"            obtenerDescripcionTranscciones = False\r\n",
							"            obtenerCostoTranscciones = True\r\n",
							"        else:\r\n",
							"            if row[\"contenido\"] != \"\":\r\n",
							"                ArrayDescripcionTransacciones.append((iteradorDescripcionTranscciones, row[\"contenido\"]))\r\n",
							"                iteradorDescripcionTranscciones = iteradorDescripcionTranscciones + 1\r\n",
							"\r\n",
							"    if (obtenerCostoTranscciones == True):\r\n",
							"        if (row[\"contenido\"] == \"\" and iteradorCostoTranscciones != 1):\r\n",
							"            obtenerCostoTranscciones = False\r\n",
							"        else:\r\n",
							"            if row[\"contenido\"] != \"\":\r\n",
							"                Valor = row[\"contenido\"]\r\n",
							"                Valor = Valor.replace(\",\", \"\")\r\n",
							"                if \"-\" in Valor:\r\n",
							"                    Valor = Valor.replace(\"-\", \"\")                    \r\n",
							"                    Valor = \"-\"+Valor\r\n",
							"                Valor = float(Valor)\r\n",
							"                ArrayCostoTransacciones.append((iteradorDescripcionTranscciones, Valor))\r\n",
							"                iteradorCostoTranscciones = iteradorCostoTranscciones + 1\r\n",
							"\r\n",
							"    if (row[\"contenido\"] == \"Autorización\"):\r\n",
							"        obtenerIDDataTranscciones = True\r\n",
							"\r\n",
							"    if (row[\"contenido\"] == \"Transacción\"):\r\n",
							"        obtenerFechaTranscciones = True\r\n",
							"\r\n",
							"    if (row[\"contenido\"] == \"Valor Original\"):\r\n",
							"        obtenerDescripcionTranscciones = True\r\n",
							"\r\n",
							""
						],
						"outputs": [],
						"execution_count": 204
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#Crear dataframes\r\n",
							"\r\n",
							"df_idTransacciones = spark.createDataFrame(ArrayIdTransacciones, schema1)\r\n",
							"df_FechaTransacciones = spark.createDataFrame(ArrayFechaTransacciones, schema2)\r\n",
							"df_DescripcionTransacciones = spark.createDataFrame(ArrayDescripcionTransacciones, schema3)\r\n",
							"df_CostoTransacciones = spark.createDataFrame(ArrayCostoTransacciones, schema4)"
						],
						"outputs": [],
						"execution_count": 205
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_idTransacciones.show()\r\n",
							"df_FechaTransacciones.show()\r\n",
							"df_DescripcionTransacciones.show()\r\n",
							"df_CostoTransacciones.show()"
						],
						"outputs": [],
						"execution_count": 206
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Iterate over the elements in the array\r\n",
							"for row in df_fromated.collect():\r\n",
							"    #if (row[\"contenido\"] == \"Autorización\"):\r\n",
							"        print(row[\"contenido\"])\r\n",
							""
						],
						"outputs": [],
						"execution_count": 155
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Reading PDF File - Test Concept')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SandBoxScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "c90baa01-a231-47f8-9d52-8eb2a1e7c0f0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"tags": []
						},
						"source": [
							"#Procesamiento con PyPDF2\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from io import BytesIO\r\n",
							"import PyPDF2\r\n",
							"\r\n",
							"pdf_path = \"abfss://stagefiles@datalakeproyectomaestria.dfs.core.windows.net/Extracto_7358651_202108_TARJETA_VISA_2166.pdf\"\r\n",
							"pdf_password = \"1233491047\"\r\n",
							"\r\n",
							"# Read the PDF file as bytes\r\n",
							"pdf_bytes = spark.read \\\r\n",
							"    .format(\"binaryFile\") \\\r\n",
							"    .option(\"path\", pdf_path) \\\r\n",
							"    .load() \\\r\n",
							"    .select(\"content\") \\\r\n",
							"    .collect()[0][0]\r\n",
							"\r\n",
							"# Create a BytesIO stream from the PDF bytes\r\n",
							"pdf_stream = BytesIO(pdf_bytes)\r\n",
							"pdf_stream.seek(0)  # Ensure the stream is at the beginning\r\n",
							"\r\n",
							"# Create a PdfReader object from the PDF stream\r\n",
							"pdf_reader = PyPDF2.PdfReader(pdf_stream)\r\n",
							"\r\n",
							"# Check if the PDF file is encrypted (password-protected)\r\n",
							"if pdf_reader.is_encrypted:\r\n",
							"    # Decrypt the PDF file with the provided password\r\n",
							"    pdf_reader.decrypt(pdf_password)\r\n",
							"\r\n",
							"# Extract text from the PDF pages\r\n",
							"pdf_content = \"\"\r\n",
							"for page_num in range(len(pdf_reader.pages)):\r\n",
							"    page = pdf_reader.pages[page_num]\r\n",
							"    pdf_content += page.extract_text()\r\n",
							"\r\n",
							"print(pdf_content)\r\n",
							"\r\n",
							"# Perform further processing or analysis on the content\r\n",
							"# For example, you can save it to a storage location or perform text mining operations.\r\n",
							""
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#PROCESAMIENTO CON Rapid Miner\r\n",
							"\r\n",
							"from pyspark.sql import SparkSession\r\n",
							"from io import BytesIO\r\n",
							"from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\r\n",
							"from pdfminer.pdfpage import PDFPage\r\n",
							"from pdfminer.converter import TextConverter\r\n",
							"from pdfminer.layout import LAParams\r\n",
							"import io\r\n",
							"\r\n",
							"spark = SparkSession.builder \\\r\n",
							"    .appName(\"PDF Extraction\") \\\r\n",
							"    .getOrCreate()\r\n",
							"\r\n",
							"pdf_path = \"abfss://stagefiles@datalakeproyectomaestria.dfs.core.windows.net/Extracto_7358651_202108_TARJETA_VISA_2166.pdf\"\r\n",
							"pdf_password = \"1233491047\"\r\n",
							"\r\n",
							"# Read the PDF file as bytes\r\n",
							"pdf_bytes = spark.read \\\r\n",
							"    .format(\"binaryFile\") \\\r\n",
							"    .option(\"path\", pdf_path) \\\r\n",
							"    .load() \\\r\n",
							"    .select(\"content\") \\\r\n",
							"    .collect()[0][0]\r\n",
							"\r\n",
							"# Create a BytesIO stream from the PDF bytes\r\n",
							"pdf_stream = BytesIO(pdf_bytes)\r\n",
							"pdf_stream.seek(0)  # Ensure the stream is at the beginning\r\n",
							"\r\n",
							"# Create PDFResourceManager object and set parameters\r\n",
							"rsrcmgr = PDFResourceManager()\r\n",
							"retstr = io.StringIO()\r\n",
							"\r\n",
							"# Create TextConverter object for the PDF page interpretation\r\n",
							"laparams = LAParams()\r\n",
							"device = TextConverter(rsrcmgr, retstr, laparams=laparams)\r\n",
							"\r\n",
							"# Create PDFPageInterpreter object\r\n",
							"interpreter = PDFPageInterpreter(rsrcmgr, device)\r\n",
							"\r\n",
							"# Process each page of the PDF file\r\n",
							"password = pdf_password.encode()\r\n",
							"pages = PDFPage.get_pages(pdf_stream, password=password, check_extractable=True)\r\n",
							"for page in pages:\r\n",
							"    interpreter.process_page(page)\r\n",
							"\r\n",
							"# Get the extracted text from the StringIO object\r\n",
							"pdf_content = retstr.getvalue()\r\n",
							"\r\n",
							"# Close the StringIO objects\r\n",
							"device.close()\r\n",
							"retstr.close()\r\n",
							"\r\n",
							"print(pdf_content)\r\n",
							"\r\n",
							"\r\n",
							"# Perform further processing or analysis on the content\r\n",
							"# For example, you can save it to a storage location or perform text mining operations.\r\n",
							""
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/writte a file')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"folder": {
					"name": "SandBoxScripts"
				},
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "spproymaestria",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "aabd014b-6f70-4d0c-a1bd-7dea8ca21cc0"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/01c2d20c-2c85-4121-8018-3a801e74d84c/resourceGroups/ProyectoMaestria/providers/Microsoft.Synapse/workspaces/synapseproyectomaestria/bigDataPools/spproymaestria",
						"name": "spproymaestria",
						"type": "Spark",
						"endpoint": "https://synapseproyectomaestria.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spproymaestria",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.3",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
							"data2 = [(\"James\",\"Smith\",\"Joe\",\"4355\",\"M\",3000),\r\n",
							"    (\"Michael\",\"Rose\",\"Edward\",\"40288\",\"F\",4000)\r\n",
							"  ]\r\n",
							"\r\n",
							"schema = StructType([ \\\r\n",
							"    StructField(\"firstname\",StringType(),True), \\\r\n",
							"    StructField(\"middlename\",StringType(),True), \\\r\n",
							"    StructField(\"lastname\",StringType(),True), \\\r\n",
							"    StructField(\"id\", StringType(), True), \\\r\n",
							"    StructField(\"gender\", StringType(), True), \\\r\n",
							"    StructField(\"salary\", IntegerType(), True) \\\r\n",
							"  ])\r\n",
							" \r\n",
							"df = spark.createDataFrame(data=data2,schema=schema)\r\n",
							"\r\n",
							"df.write.csv(\"abfss://stagefiles@datalakeproyectomaestria.dfs.core.windows.net/validate_permissions.csv\")"
						],
						"outputs": [],
						"execution_count": 5
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sparkConfiguration1')]",
			"type": "Microsoft.Synapse/workspaces/sparkConfigurations",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"configs": {
					"spark.jars.packages": "org.apache.tika:tika-parsers:1.26"
				},
				"created": "2023-05-30T23:19:10.1900000-05:00",
				"createdBy": "brallamsantiago.morales334@comunidadunir.net",
				"annotations": [],
				"configMergeRule": {
					"artifact.currentOperation.spark.jars.packages": "replace"
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/spproymaestria')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 10
				},
				"autoScale": {
					"enabled": false,
					"maxNodeCount": 19,
					"minNodeCount": 3
				},
				"nodeCount": 3,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.3",
				"libraryRequirements": {
					"content": "name: proyectomaestiraenv\r\nchannels:\r\n- defaults\r\ndependencies:\r\n- pip:\r\n  - Tika\r\n  - PyPDF2\r\n  - pdfminer",
					"filename": "enviroment.yml",
					"time": "2023-06-06T05:12:59.1874798Z"
				},
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sqlpoolproyectomaestria')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}